{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62b79d4",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Convolutional Neural Network\"\n",
    "date:   2023-03-09 10:14:54 +0700\n",
    "categories: jekyll update\n",
    "---\n",
    "\n",
    "# TOC\n",
    "\n",
    "- [Definition](#define)\n",
    "- [Code example](#code)\n",
    "\n",
    "\n",
    "# Definition <a name=\"define\"></a>\n",
    "\n",
    "Remember the dense neural net as a linear combination of input features plus a nonlinear transformation (an activation function). For this kind of neural net, we need to flatten the input into a one-dimensional array. One thing about one dimensional array is that it doesn't imply 2-dimensional quality. Meanwhile there are a lot of data in this world that inherently has a structure of more than one dimension. For example, a colored image has 3 channels, each channel has a single-colored image with width w x height h (pixels). So this image's matrix interpretation would be a matrix of 3 dimensions: 3 w * h matrices. Moreover, if we flatten this image out to feed into a dense network like we did before, the amount of connection and parameters can become millions quite fast. So the convolution neural net (CNN) has much less parameters given the same amount of neurons and hidden layers, making it easier to train and compute.\n",
    "\n",
    "\n",
    "## Convo layer\n",
    "\n",
    "To be able to process such complex data, we use convolutional neural network. We would then use matrix multiplication to transform the input (or convolute). The function that we would use to transform those input is called a filter or kernel, effectively a matrix of small size (usually 3x3). The filter is full of weights ($$ \\theta $$) as we know and it would transform the entire input matrix by sliding from top left to the bottom right horizontally. The sliding can take one, two or more values per step (which is called stride), hence it reduces the original input's data. Since sliding with a matrix loses some values untransformed at the borders, we pad the original input matrix with 0 around it so that the algorithm leaves nothing untouched. The result after being slided would be added a bias ($$ \\theta_0 $$) and then activated. This is just like machine learning models as we know it, though I figured the bias addition and activation could be done after the pooling layer explained below. Each filter would then create an output called an activation map. Or we can say that it contribute to one depth of it. A convolution layer with 10 filters (kernels) would result in an activation map with depth of 10. Each depth carries a simple information about the original input, as we explain shortly below.\n",
    "\n",
    "Those filters at the beginning layers of the neural net are dubbed low-level neurons because it resembles the mechanism of biological simple neurons (or locally receptive neurons): in the brain, there would be simple neurons that are in charge of identifying very simple shapes such as a horizontal line or a vertical line. Those simple signals then would be sent to groups of more complex neurons and those group would be able to combine the lines  to recognize more abstract patterns (corners, wrinkles, eyes, noses, etc).\n",
    "\n",
    "Those filters can also be seen as the visual perception of the neurons in the activation maps. The perception is small, each of those neurons would be connected not to all but to a limited part of the input hence the word locally receptive. We end up with one neuron per pixel and lots of activation maps for each layer. \n",
    "\n",
    "## Pooling layer\n",
    "\n",
    "After being convoluted, we can add a pooling layer to the data. A pooling matrix is usually 2x2 (up to 5x5) and it works by choosing one of the value in that window, mostly the max or sometimes the average. This means to reduce the matrix size, hence reduce complexity, computation and overfit. The depth of the activation map would be the same though. Gradient descent flows through this pooling normally (through the max data value).\n",
    "\n",
    "\n",
    "## Dropout layer\n",
    "\n",
    "Since the dimensions can grow big, there is a technique called dropout that temporarily blank out a percentage of neurons. Think of this analogy: in a company, when a person is absent, others would learn to take over his job. In general, with regular absence, the employees become more generalized in their skills and this overall is better for the company. Also for the neural net, since it becomes less overfit.\n",
    "\n",
    "# Code example <a name=\"code\"></a>\n",
    "\n",
    "In this example, let's explore the fashion MNIST (similar to MNIST but for fashion images). First, let's add an extra dimension for the data, it would store the number of data points we have and together we have a tensor (multidimensional vector) for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddeee7f",
   "metadata": {},
   "source": [
    "A CNN architecture is more complicated than a dense net. Firstly, we put a convolution layer (usually with relu activation), then a max pool, then another convolution layer (can be with double neurons, to process information better), another max pool, and so on. After that, when we are done with the convoluted layers, when we feel like the net is enough to comprehend complex feature of the data, we move on to the dense layers. Before doing that, we need to add a flatten method which is not a layer, it is just an unwinding of neurons from the convolution layer. Then we put some dense layer, mix with dropout (rate typically 0.2 to 0.5). The last layer, as usual, is a softmax for the classification problem, and relu for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
    "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:10]  # pretend we have new images\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a31cb",
   "metadata": {},
   "source": [
    "Epoch 1/10\n",
    "\n",
    "1719/1719 [==============================] - 240s 136ms/step - loss: 0.7840 - accuracy: 0.7094 - val_loss: 0.3669 - val_accuracy: 0.8788\n",
    "\n",
    "Epoch 2/10\n",
    "\n",
    "1719/1719 [==============================] - 229s 133ms/step - loss: 0.4747 - accuracy: 0.8292 - val_loss: 0.2914 - val_accuracy: 0.8920\n",
    "\n",
    "Epoch 3/10\n",
    "\n",
    "1719/1719 [==============================] - 226s 132ms/step - loss: 0.3912 - accuracy: 0.8591 - val_loss: 0.2578 - val_accuracy: 0.9042\n",
    "\n",
    "Epoch 4/10\n",
    "\n",
    "1719/1719 [==============================] - 226s 132ms/step - loss: 0.3450 - accuracy: 0.8765 - val_loss: 0.2484 - val_accuracy: 0.9084\n",
    "\n",
    "Epoch 5/10\n",
    "\n",
    "1719/1719 [==============================] - 230s 134ms/step - loss: 0.3077 - accuracy: 0.8894 - val_loss: 0.2387 - val_accuracy: 0.9096\n",
    "\n",
    "Epoch 6/10\n",
    "\n",
    "1719/1719 [==============================] - 231s 134ms/step - loss: 0.2845 - accuracy: 0.8979 - val_loss: 0.2483 - val_accuracy: 0.9092\n",
    "Epoch 7/10\n",
    "\n",
    "1719/1719 [==============================] - 230s 134ms/step - loss: 0.2568 - accuracy: 0.9071 - val_loss: 0.2445 - val_accuracy: 0.9184\n",
    "\n",
    "Epoch 8/10\n",
    "\n",
    "1719/1719 [==============================] - 233s 136ms/step - loss: 0.2368 - accuracy: 0.9129 - val_loss: 0.2285 - val_accuracy: 0.9186\n",
    "\n",
    "Epoch 9/10\n",
    "\n",
    "1719/1719 [==============================] - 228s 133ms/step - loss: 0.2233 - accuracy: 0.9188 - val_loss: 0.2314 - val_accuracy: 0.9162\n",
    "\n",
    "Epoch 10/10\n",
    "\n",
    "1719/1719 [==============================] - 232s 135ms/step - loss: 0.2075 - accuracy: 0.9234 - val_loss: 0.2334 - val_accuracy: 0.9182\n",
    "313/313 [==============================] - 11s 37ms/step - loss: 0.2620 - accuracy: 0.9158\n",
    "1/1 [==============================] - 0s 142ms/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeaa10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aea6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e5140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
