{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71173cb2",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Recurrent Neural Network\"\n",
    "date:   2023-03-17 14:14:54 +0700\n",
    "categories: jekyll update\n",
    "---\n",
    "\n",
    "## Encoder - decoder\n",
    "\n",
    "To encode is to turn words/images into numbers (digitalize) so that we can do matrix multiplication. With number, we can also construct a data space in which closer data points means similar meaning in the real world. To decode is to turn those numbers back into words/images. Through encoder-decoder, words can be translated to another langauges, images can be worked on more efficiently.\n",
    "\n",
    "# Attention layer\n",
    "\n",
    "Remember the architect of recurrent neural network (RNN)? It is a neuron that wires the previous output to the next input. For translation the translation task in general, we need to output a sequence of words. We also need a context for those words, so that context is translated into context, too. Therefore an attention layer is added to the RNN. This attention layer can be called a context vector. As the name suggests, it carries the context, i.e. let each word know which other words it needs to pay attention to after lots of training together. This would make the output words understand its position. And words that are together would carry higher weights in attenting to each other. The calculation for the attention layer is as follows:\n",
    "\n",
    "- First, at each step, we calculate score for each word in related to all each words (including itself), using dot product or cosine similarity. This results in numbers saying how much is a word related to another word. Of course a word pays the most attention to itself.\n",
    "\n",
    "- Let the scores run through a softmax function to turn them into probability distribution of the attention weights.\n",
    "\n",
    "- Combine the weights with hidden state result to have context vector\n",
    "\n",
    "- Concatenate the context vector and the hidden state vector. Transform the concanated vector with a weight vector. Activate it via the tanh function. That gives us the attention vector.\n",
    "\n",
    "# Transformer\n",
    "\n",
    "Even though LSTMs are useful, transformer is the next generation translator in which they use attention layer only. \n",
    "\n",
    "## The encoder\n",
    "\n",
    "- Words are turned into vector of numbers. This forms an input \n",
    "matrix with the number of rows to be the length of the sentence (each word is a row). The number of columns are the dimensions of the word vector. More dimensions mean we can represent more complex meaning of the words, but more expensive computation. Plus sometimes it is not necessary to be so complex. \n",
    "\n",
    "- Positional encoding: since transformer only uses attention, it doesn't use any of the sequence analysis provided by RNN, we can shuffle the sentence and it would still predict true. To give it the sense of positions for each word, \n",
    "\n",
    "- Stack the row vectors together and we have the matrix embeding of the sentence. Multiply the matrix with 3 matrices to have a result of 3 corresponding matrices: Q (query), K (key), and V (value). Here we have the digital representation of 3 versions of the original sentence in which we vaguely understand the purpose of the sentence (query matrix), the keys of the sentence and the value of the sentence (again, each word is one row).\n",
    "\n",
    "- For each word (for each row), we multiply query row with key row. The paper by Google suggests to divide this number to squareroot of dimensions of keys, in case the result becomes too big. This is the attention score for each word in relation with all each words. These scores are then normalized to sum up to 1. This softmax mutiplies with value with create a convex combination of value (or weighted value). This is the attention vector for each word. As usual, more dimensions of this vector means more meaning representation. \n",
    "\n",
    "- We just finish the calculation for one head of the attention. Usually we do multiple head. At the end, we concanate those heads into a matrix. Take another matrix W, so that the output matrix has the same dimension as the input matrix.\n",
    "\n",
    "- At that output, each new word still is a row. Now that output matrix would let each word to go through fully connectec neurons so that the number of dimensions are kept. We have the final output of an encoder block at this point.\n",
    "\n",
    "- We can repeat this process multiple time, to have multiple encoding block (or layer).\n",
    "\n",
    "## The decoder\n",
    "\n",
    "- The decoding process would go from left to right, like time, and there would be no use of future words since that would be cheating time.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb463a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
