---
layout: post
title:  "RL: RLHF"
date:   2023-06-21 10:14:54 +0700
categories: DeepLearning
---


# Introduction


Language models, the computational tools capable of understanding, generating, and enhancing human-like text, have grown exponentially in their capabilities in recent years. These models can now produce text that not only mimics human language patterns but also exhibits creativity and diversity based on the input prompts provided by the user. However, defining what constitutes "good" or high-quality text generated by these models can be quite complex due to the subjective and context-specific nature of language.

For instance, there are numerous situations where different attributes of the generated text are desired. When writing stories, we want the language model to generate creative and engaging content. In contrast, if the task is to produce informative pieces of text, we need it to be factual and accurate. Similarly, in cases where code snippets are being generated, it's essential for the code to be functional and executable.

Creating a loss function (a measure of how far the model's predictions are from the actual outcomes) that can encapsively capture all these distinct attributes seems nearly impossible. Consequently, most of the language models as of now continue to be trained using a relatively straightforward next token prediction loss, such as cross-entropy. This technique involves predicting the next word or token in a sentence based on the words or tokens that have come before it.

However, this loss function doesn't fully account for the quality of the generated text in terms of its alignment with human preferences. As a solution, specific metrics like BLEU (Bilingual Evaluation Understudy) or ROUGE (Recall-Oriented Understudy for Gisting Evaluation) are used. These metrics, designed to better capture human preferences, compare the generated text with reference texts using specific rules, providing a more nuanced evaluation of the model's performance. However, they are still limited in their scope and do not fully capture the complexity of human language preferences.

The concept of Reinforcement Learning from Human Feedback (RLHF) presents a promising alternative. It suggests using human feedback on the generated text as a measure of performance. Taking it a step further, RLHF proposes using this human feedback as a loss function to optimize the model. This approach leverages methods from reinforcement learning, a subfield of machine learning where an agent learns to make decisions by performing certain actions in an environment to maximize some notion of cumulative reward.




Reinforcement Learning with Human Feedback (RLHF) leverages human knowledge and expertise in the learning process. An RL agent would be trained with feedback from a human, who could either be an expert or not. The feedback provided by the human could be used to guide the agent and it can be as simple as.a reward or as complex as demonstration or correction. The process follows a few steps: first, an initial policy is trained with standard RL methods. Second, the policy is then shown to a human who would give additional feedbacks on the agent's actions. This feedback could be binary comparison of trajectories, ratings on a numerical scale, or explicit corrections. The feedback is meant to improve the policy and can be incorporated as another RL process that maps state-action pairs to human ratings. After several iterations, the policy is expected to improve substantially. By incorporating human feedback, we can guide the agent to good policies more safely and efficiently.







By directly optimizing the language model based on human feedback, RLHF aligns the training of the model more closely with complex human values. The approach holds significant potential to enhance the quality of generated text in line with human preferences and context-specific requirements. The advent of RLHF thus marks a significant stride towards improving the adaptability and performance of language models, opening up exciting avenues for future research and applications.
