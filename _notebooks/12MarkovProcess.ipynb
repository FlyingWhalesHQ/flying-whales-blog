{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2cc852",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Markov Decision Process (MDP)\"\n",
    "date:   2022-07-13 10:14:54 +0700\n",
    "categories: MachineLearning\n",
    "---\n",
    "\n",
    "# TOC\n",
    "\n",
    "- [Definition](#define)\n",
    "- [Example](#ex)\n",
    "- [Code example](#code)\n",
    "\n",
    "# Definition <a name=\"#define\"></a>\n",
    "\n",
    "A Markov decision process is an agent that has access to the following information:\n",
    "\n",
    "- state space S\n",
    "\n",
    "- action set A in each state s\n",
    "\n",
    "- transition probabilities P over the state space at each state (i.e. from one state we know the probability to end up in the next state if we take action a)\n",
    "\n",
    "- discount factor $$ \\gamma $$ to discount future cashflow\n",
    "\n",
    "- reward function R over the action a and the state s it ends up in\n",
    "\n",
    "A policy is to map each state to an action. The utility of a policy is the discounted sum of all the rewards on the path of that policy. We discount the value of tomorrow so that money of today worths a bit more than that same amount tomorrow. Here is the discounted sum:\n",
    "\n",
    "$$ u = r_1 + \\gamma r_2 + \\gamma^2 r_3 + ... $$\n",
    "\n",
    "The value of a policy at a state is the expected utility $$ V_{\\pi}(s) $$. The Q-value $$ Q_{\\pi} (s,a) $$ is the expected utility of taking action a at state s, and then following policy $$ \\pi $$. Value at s is either equals 0 (if it is the end), or equals its Q-value otherwise, with Q-value to be the total of probable transitions to all the s' multiplied with its discounted cashflow:\n",
    "\n",
    "$$ V_{\\pi}(s) =\n",
    "\\begin{cases}\n",
    "    0 & \\text{if s ends} \\\\\n",
    "    Q_{\\pi}(s,\\pi(s)) & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n",
    "with $$ Q_{\\pi}(s,a)=\\sum_{s'} P(s,a,s') E{[R(s,a,s') + \\gamma V_{\\pi} (s')]} $$ (Q-value equals probabilities multiplied by expected value). To evaluate policy, we initialize values at all states to be 0:\n",
    "\n",
    "$$ V_{\\pi}^{(0)} \\leftarrow 0 $$ \n",
    "\n",
    "Then for each iteration:\n",
    "\n",
    "$$ V_{\\pi}^{(t)}(s) \\leftarrow Q^{t-1}(s,\\pi(s)) = \\sum_{s'} P(s,\\pi(s), s') E {[R(s,\\pi(s), s') + \\gamma V_{\\pi}^{(t-1)} (s')]} $$\n",
    "\n",
    "We iterate until:\n",
    "\n",
    "$$ max_{s \\in S} \\mid V_{\\pi}^{(t)} (s) - V_{\\pi}^{(t-1)}(s) \\mid \\leq \\epsilon $$\n",
    "\n",
    "The optimal value $$ V_{opt}(s) $$ is the maximum value for each policy. As above,\n",
    "\n",
    "$$ V_{opt}(s) =\n",
    "\\begin{cases}\n",
    "    0 & \\text{if s ends} \\\\\n",
    "    max_{a \\in A(s)} Q_{opt}(s,a) & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n",
    "with $$ Q_{opt}(s,a)=\\sum_{s'} P(s,a,s') E{[R(s,a,s') + \\gamma V_{opt} (s')]} $$\n",
    "\n",
    "Following the similar vein, the optimal policy would be the one that maximize the Q-value with action a:\n",
    "\n",
    "$$ \\pi_{opt}(s) = arg max_{a \\in A(s)} Q_{opt}(s,a) $$\n",
    "\n",
    "Now we iterate for optimal value:\n",
    "\n",
    "- Initialize $$ V_{opt}^{(0)}(s) \\leftarrow 0 $$\n",
    "\n",
    "- For each state s: $$ V_{opt}^{(t)} \\leftarrow  max_{a \\in A(s)} Q_{opt}^{(t-1)} (s,a) =  max_{a \\in A(s)} \\sum_{s'} P(s,a,s') E{[R(s,a,s') + \\gamma V_{opt}^{(t-1)} (s')]} $$\n",
    "\n",
    "# Example <a name=\"#ex\"></a>\n",
    "\n",
    "We play a game. At each round, you choose to stay or quit. If you quit, you get $$ \\$10 $$ and ends the game. If you stay, you get $$ \\$4 $$ and $$ \\frac{1}{3} $$ probability of ending the game and $$ \\frac{2}{3} $$ probability of going to the next round. Let $$ \\gamma = 1 $$.\n",
    "\n",
    "There are two policies: to stay or to quit. The value of policy \"quit\" is $$ \\$10 $$. Let's evaluate the policy of \"stay\":\n",
    "\n",
    "$$ V_{\\pi} (end) = 0 $$\n",
    "\n",
    "$$ V_{\\pi}(in) = \\frac{1}{3} (4 + V_{\\pi} (end)) + \\frac{2}{3} (4 + V_{\\pi}(in)) = 4 + \\frac{2}{3} V_{\\pi}(in) $$\n",
    "\n",
    "$$ \\Leftrightarrow \\frac{1}{3} V_{\\pi}(in) = 4 $$\n",
    "\n",
    "$$ \\Leftrightarrow V_{\\pi}(in) = 12 > 10 $$\n",
    "\n",
    "We definitely should stay in the game.\n",
    "\n",
    "# Code example <a name=\"code\"></a>\n",
    "\n",
    "At time 0, we set value policy \"stay\" to be 0. At iteration 1, value (in) = Q-value at 1 = probabilities * expected utility. delta to be the absolute difference between value of previous iteration minus the value of this iteration. If delta is smaller than 0.001, we stop the calculation. As you will see below, the calculation stops at iteration 20, and we have value of policy \"stay\" to be 11.99 $$ \\approx $$ 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67732716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.0\n",
      "1 6.666666666666666\n",
      "2 8.444444444444445\n",
      "3 9.62962962962963\n",
      "4 10.419753086419753\n",
      "5 10.946502057613168\n",
      "6 11.297668038408778\n",
      "7 11.53177869227252\n",
      "8 11.687852461515012\n",
      "9 11.791901641010009\n",
      "10 11.86126776067334\n",
      "11 11.907511840448892\n",
      "12 11.938341226965928\n",
      "13 11.958894151310618\n",
      "14 11.972596100873746\n",
      "15 11.98173073391583\n",
      "16 11.98782048927722\n",
      "17 11.991880326184814\n",
      "18 11.99458688412321\n",
      "19 11.99639125608214\n",
      "20 11.997594170721426\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "V = 0\n",
    "delta = 0\n",
    "for i in range (100):\n",
    "    v = V\n",
    "    V = 1/3 * (4 + 0) + 2/3 * (4 + V)\n",
    "    delta = np.abs(v-V)\n",
    "    if delta < 0.001:\n",
    "        break\n",
    "    print(i,V)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
