{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060f8211",
   "metadata": {},
   "source": [
    "# Object detection\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Speaking of image recognition tasks, we can distinguish them based on the nature of the task. For example, image classification is a task that labels images. Object localization is to draw a bounding box around one or multiple objects in the image. Object detection is a more difficult task that combines both tasks: to draw a bounding box and then label it. This task is critical for a wide range of applications, including surveillance, autonomous vehicles, robotics, and medical imaging.\n",
    "\n",
    "Localizing an object in an image can be a regression task. We can predict a bounding box around the object, for example the coordinates of the center, plus its height and width. The measures are normalized such that the coordinates are in the range of 0 and 1. Another common version is to predict the square root of the height and width rather, so that a 10-pixel error of large box would be penalized less than the same error for a smaller box.\n",
    "\n",
    "Another important and related problem in computer vision is the segmentation problem. That includes object segmentation and instance segmentation. Object segmentation involves identifying the pixels that belong to an object in an image, while instance segmentation goes a step further and differentiates between multiple instances of the same object. For example, in object segmentation, the algorithm needs to distinguish human from car. In instance segmentation, the algorithm needs to distinguish different humans from each other.\n",
    "\n",
    "## IoU\n",
    "\n",
    "IoU stands for Intersection over Union, which is a measure of the overlap between two sets. In the context of object detection, IoU is often used to evaluate the accuracy of a model's predictions by comparing the predicted bounding boxes with the ground truth bounding boxes. IoU is calculated as the ratio of the intersection of the predicted and ground truth bounding boxes to the union of the same boxes. It is expressed as a value between 0 and 1, where a value of 1 indicates a perfect overlap between the two boxes, while a value of 0 indicates no overlap at all. It measures how much area of the prediction is correct, regarding the union.\n",
    "\n",
    "$$ IoU = \\frac{\\text{Area of overlap}}{\\text{Area of union}} $$\n",
    "\n",
    "## Sliding CNN\n",
    "For multiple objects, there is a sliding CNN approach to this by sliding a CNN across the image grid and make prediction at each step. The method has one drawback, it can detect an object multiple times with multiple bounding boxes. To remedy this, we can use non max suppression technique. We set some threshold for the object score and remove all the bounding boxes with less than that score. Find the highest object score bounding box, and remove all remaining bounding boxes that overlap it a lot (with an IoU bigger than 60%).\n",
    "\n",
    "## Region based CNN\n",
    "\n",
    "The paper \"Rich feature hierarchies for accurate object detection and semantic segmentation\" - 2014 proposes an object detection system consists of three modules. The first generates region proposals (around 2000 regions). The second module is a CNN that extracts a fixed length feature vector from each region. The third module is linear SVMs. The first module uses selective search. \n",
    "\n",
    "The selective search generates small region of interests, then recursively combine adjacent regions. It considers four types of similarity when combining the smaller segmentation into larger ones:\n",
    "\n",
    "- Color similarity: a color histogram of 25 bins is calculated for each channel then concanated into a color vector of 75 dimensions. Color similarity of two regions is then the histogram intersection: $$ S_{color} (r_i, r_j) = \\sum_{k=1}^{n} min(c_i^k, c_j^k) $$ with $$ c_i^k, c_j^k $$ to be the histogram value for kth bin.\n",
    "\n",
    "- Texture similarity: We extract Gaussian derivatives for 8 orientations in each channel, then a 10 bin historgram for each orientation and color channel. Texture similarity of two regions is then the intersection of histograms $$ S_{texture}(r_i, r_j) = \\sum_{k=1}^{n} min(t_i^k, t_j^k) $$ with $$ t_i^k $$ being the histogram value of kth bin in the resulting vector.\n",
    "\n",
    "- Size similarity: This similarity encourages smaller regions to merge eraly. $$ S_{size} (r_i, r_j) = 1 - \\frac{size(r_i) + size(r_j)}{size(im)} $$ with size(im) being the size of the image in pixels.\n",
    "\n",
    "- Fill similarity: This measures how well two regions fit. $$ s_{fill}(r_i,r_j) = 1 - \\frac{size(BB_{ij} - size(r_i) -size(r_j)}{size(im)} $$  with size(BB) is the bounding box around region i and j.\n",
    "\n",
    "- The final similarity is a linear combination of those four similarities above: $$ s(r_i, r_j) = a.S_{color}(r_i,r_j) + b.S_{texture}(r_i,r_j) + c.S_{size}(r_i,r_j) + d.S_{fill}(r_i, r_j) $$\n",
    "\n",
    "After the region proposals, the authors do feature extraction by extracting a 4096 dimensional feature vector from each region with a CNN of 5 convo layers and 2 dense layers, inputing 227 x 227 pixels. They warp all inputs into 227 x 227 size regardless of aspect ratio and original size.\n",
    "\n",
    "## Fully CNN\n",
    "\n",
    "Semantic segmentation is to classify each pixel in an image to its class of object. In a fully CNN, the dense layers at the top are replaced with CNN. An example of a CNN that can input 448 x 448 images and output 10 numbers:\n",
    "\n",
    "- Number 0 to 4 are put through a softmax function and turned into the class probabilities\n",
    "\n",
    "- Number 5 is sent through the sigmoid function and gives the object score\n",
    "\n",
    "- Number 6 and 7 are the bounding box's center coordinates. They go through a sigmoid function to be scaled back into 0 and 1.\n",
    "\n",
    "- Number 8 and 9 are the bounding box's height and width.\n",
    "\n",
    "This CNN can be converted into a fully CNN with the last layer to be a CNN instead of a dense layer. It will output 8x8 predictions.\n",
    "\n",
    "<img width=\"597\" alt=\"Screen Shot 2023-04-17 at 20 19 34\" src=\"https://user-images.githubusercontent.com/7457301/232496269-5c632b41-e4a0-489f-b3ee-298798c52401.png\">\n",
    "\n",
    "\n",
    "## You only look once (YOLO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a23db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=yolov5s.pt, source=../object2.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /Users/nguyenlinhchi/Documents/GitHub/flying-whales-blog/_notebooks/requirements.txt not found, check failed.\n",
      "YOLOv5 ðŸš€ v7.0-145-g94714fe Python-3.9.13 torch-2.0.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "image 1/1 /Users/nguyenlinhchi/Documents/GitHub/flying-whales-blog/_notebooks/object2.jpg: 448x640 3 persons, 2 bowls, 250.1ms\n",
      "Speed: 2.5ms pre-process, 250.1ms inference, 12.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source ../object2.jpg  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573680eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
