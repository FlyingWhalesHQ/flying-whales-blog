{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf747c5",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Conditional GAN (cGAN and pix2pix)\"\n",
    "date:   2023-03-23 10:14:54 +0700\n",
    "categories: jekyll update\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Generative Adversarial Network is a network with two game players: a generator that generates fake images and a discriminator that spots the real from the fake images. The network is trained and ultimately they reach an equilibrium in which the generator generates images close to real art, and the discriminator cannot distinguish the forged version. This network results in sophisticated generator and the generator is usually used for generating real images, even art. There are many variants of GANs since the original paper.\n",
    "\n",
    "The normal GAN doesn't support generating images of a certain category, it only generates a general beautiful images, of any categories it was trained on. Conditional GAN is a variant of GANs that has an additional embeded vector y as input for both generator and discriminator. Trained in this way, a cGAN generator can generate images of a given category.\n",
    "\n",
    "In cGAN, the value function becomes dependent on the input y (hence the word conditional):\n",
    "\n",
    "$$ min_G max_D V(D,G) = E_x {[log D(x \\mid y)]} + E_z {[log(1-D(G(z \\mid y)))]} $$\n",
    "\n",
    "Apart from generating images of a certain kind, cGAN paper also introduces an application in multi-modal learning, in which it generates tags (text) for image captioning. This is multimodal since the number of tags for one image can be more than one, and those words can also be synonym.\n",
    "\n",
    "There are several ways we can add the label y to the generator:\n",
    "\n",
    "- As an embedding layer\n",
    "\n",
    "- Add as an additional channel to the images\n",
    "\n",
    "- keep embedding dimension low then upsample to match image size\n",
    "\n",
    "# Code example\n",
    "\n",
    "Follows is an example of the discriminator and generator. When you run for MNIST for Fashion MNIST dataset or CIFAR-10, you need to adapt the input and some layers to fit the dimensions of the images (28x28x1 for MNISTs and 32x32x3 color images for CIFAR-10). MNISTs are black and white images (one channel, 28 pixel width and 28 pixel height) and CIFAR-10 are color images with 3 channels (RGB, and 32 pixel width and 32 pixel height). The following discriminator and generator are adapted for CIFAR-10. To adapt among different length of tensors, we can run the `model.summary()` and fix the numbers accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(32,32,3), n_classes=10):\n",
    "\t# label input\n",
    "\tin_label = Input(shape=(1,))\n",
    "\t# embedding for categorical input\n",
    "\tli = Embedding(n_classes, 50)(in_label)\n",
    "\t# scale up to image dimensions with linear activation\n",
    "\tn_nodes = in_shape[0] * in_shape[1]\n",
    "\tli = Dense(n_nodes)(li)\n",
    "\t# reshape to additional channel\n",
    "\tli = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=in_shape)\n",
    "\t# concat label as a channel\n",
    "\tmerge = Concatenate()([in_image, li])\n",
    "\t# downsample\n",
    "\tfe = Conv2D(64, (3,3), strides=(2,2), padding='same')(merge)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\t# downsample\n",
    "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
    "\t# flatten feature maps\n",
    "\tfe = Flatten()(fe)\n",
    "\t# dropout\n",
    "\tfe = Dropout(0.4)(fe)\n",
    "\t# output\n",
    "\tout_layer = Dense(1, activation='sigmoid')(fe)\n",
    "\t# define model\n",
    "\tmodel = Model([in_image, in_label], out_layer)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes=10):\n",
    "\t# label input\n",
    "\tin_label = Input(shape=(1,))\n",
    "\t# embedding for categorical input\n",
    "\tli = Embedding(n_classes, 50)(in_label)\n",
    "\t# linear multiplication\n",
    "\tn_nodes = 8 * 8\n",
    "\tli = Dense(n_nodes)(li)\n",
    "\t# reshape to additional channel\n",
    "\tli = Reshape((8, 8, 1))(li)\n",
    "\t# image generator input\n",
    "\tin_lat = Input(shape=(latent_dim,))\n",
    "\t# foundation for 7x7 image\n",
    "\tn_nodes = 128 * 8 * 8\n",
    "\tgen = Dense(n_nodes)(in_lat)\n",
    "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
    "\tgen = Reshape((8, 8, 128))(gen)\n",
    "\t# merge image gen and label input\n",
    "\tmerge = Concatenate()([gen, li])\n",
    "\t# upsample to 14x14\n",
    "\tgen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same')(merge)\n",
    "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
    "\t# upsample to 28x28\n",
    "\tgen = Conv2DTranspose(32, (4,4), strides=(2,2), padding='same')(gen)\n",
    "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
    "\t# output\n",
    "\tout_layer = Conv2D(3, (8,8), activation='tanh', padding='same')(gen)\n",
    "\t# define model\n",
    "\tmodel = Model([in_lat, in_label], out_layer)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "d_model = define_discriminator()\n",
    "d_model.summary()\n",
    "g_model = define_generator(latent_dim)\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a3dbf",
   "metadata": {},
   "source": [
    "#### Model: \"model_10\" (discriminator)\n",
    "\n",
    "|Layer (type)|Output Shape|Param #|Connected to|\n",
    "|--|---|--|--|\n",
    "| input_17 (InputLayer) | [(None, 1)] | 0  | []  |           \n",
    "| embedding_8 (Embedding) |(None, 1, 50) | 500 |['input_17[0][0]'] | \n",
    "| dense_16 (Dense) |(None, 1, 1024)|52224|['embedding_8[0][0]'] |                      \n",
    "| input_18 (InputLayer) |[(None, 32, 32, 3)]  |0 |[]|         \n",
    "| reshape_11 (Reshape)|(None, 32, 32, 1)|0 |['dense_16[0][0]'] |                                    \n",
    "| concatenate_8 (Concatenate)  |  (None, 32, 32, 4) |   0 |          ['input_18[0][0]','reshape_11[0][0]'] |        \n",
    "|conv2d_13 (Conv2D)|(None, 16, 16, 64)  | 2368  | ['concatenate_8[0][0]']  |        \n",
    "| leaky_re_lu_19 (LeakyReLU)  |   (None, 16, 16, 64) |  0  |         ['conv2d_13[0][0]']  |            \n",
    "|conv2d_14 (Conv2D)  |           (None, 8, 8, 128) |   73856    |   ['leaky_re_lu_19[0][0]']  |       \n",
    "|leaky_re_lu_20 (LeakyReLU)   |  (None, 8, 8, 128)  |  0  |         ['conv2d_14[0][0]']  |            \n",
    "|flatten_5 (Flatten)        |    (None, 8192)     |    0  |         ['leaky_re_lu_20[0][0]']    |     \n",
    "|dropout_5 (Dropout)    |        (None, 8192)  |       0  |         ['flatten_5[0][0]']     |         \n",
    "|dense_17 (Dense)        |       (None, 1)   |         8193 |       ['dropout_5[0][0]']    |          \n",
    " \n",
    "Total params: 137,141\n",
    "\n",
    "Trainable params: 137,141\n",
    "\n",
    "Non-trainable params: 0\n",
    "\n",
    "\n",
    "#### Model: \"model_11\" (generator)\n",
    "\n",
    "|Layer (type)|Output Shape|Param #|Connected to|\n",
    "|--|--|--|--|\n",
    "| input_20 (InputLayer)          |[(None, 100)]        |0           |[]|\n",
    "| input_19 (InputLayer)          |[(None, 1)]          |0           |[] |                              \n",
    "| dense_19 (Dense)               |(None, 8192)         |827392      |['input_20[0][0]'] |              \n",
    "| embedding_9 (Embedding)        |(None, 1, 50)        |500         |['input_19[0][0]']  |             \n",
    "| leaky_re_lu_21 (LeakyReLU)     |(None, 8192)         |0           |['dense_19[0][0]']   |            \n",
    "| dense_18 (Dense)               |(None, 1, 64)        |3264        |['embedding_9[0][0]'] |           \n",
    "| reshape_13 (Reshape)           |(None, 8, 8, 128)    |0           |['leaky_re_lu_21[0][0]']|         \n",
    "| reshape_12 (Reshape)           |(None, 8, 8, 1)      |0           |['dense_18[0][0]']       |        \n",
    "| concatenate_9 (Concatenate)|(None, 8, 8, 129)   | 0 |          ['reshape_13[0][0]','reshape_12[0][0]'] |  \n",
    "| conv2d_transpose_6 (Conv2DTranspose)|   (None, 16, 16, 64)|  132160  |    ['concatenate_9[0][0]'] |         \n",
    "| leaky_re_lu_22 (LeakyReLU)     |(None, 16, 16, 64) |  0   |['conv2d_transpose_6[0][0]']   |  \n",
    "| conv2d_transpose_7 (Conv2DTranspose)|  (None, 32, 32, 32) | 32800  |     ['leaky_re_lu_22[0][0]']   |      \n",
    "| leaky_re_lu_23 (LeakyReLU)     |(None, 32, 32, 32) |  0   |  ['conv2d_transpose_7[0][0]']     |\n",
    "| conv2d_15 (Conv2D)             |(None, 32, 32, 3)  |  6147|       ['leaky_re_lu_23[0][0]']        | \n",
    "                                                                                                  \n",
    "Total params: 1,002,263\n",
    "\n",
    "Trainable params: 1,002,263\n",
    "\n",
    "Non-trainable params: 0\n",
    "\n",
    "\n",
    "\n",
    "Note that the followings are after only 10 epochs:\n",
    "\n",
    "MNIST:\n",
    "![cgan_mnist](https://user-images.githubusercontent.com/7457301/227161526-8615dd68-eea1-46f7-90b6-95262dd22aae.png)\n",
    "\n",
    "FASHION MNIST:\n",
    "![cgan_fashion_mnist](https://user-images.githubusercontent.com/7457301/227161535-50e010ef-cb6a-4ab0-a30c-8e4bedd6f052.png)\n",
    "\n",
    "CIFAR:\n",
    "![cgan_cifar](https://user-images.githubusercontent.com/7457301/227167986-a4e9de9f-9d80-4f79-b302-ef4e859b71d7.png)\n",
    "\n",
    "# Pix2Pix GAN\n",
    "\n",
    "Pix2Pix is a conditional GAN architecture, used to translate image to image, for example: using a certain style (Monet, Van Gogh, ..), or from summer to winter time.\n",
    "\n",
    "\n",
    "<img width=\"1192\" alt=\"Screen Shot 2023-03-23 at 16 37 24\" src=\"https://user-images.githubusercontent.com/7457301/227162948-0cb16e4e-8d4d-466a-a0bd-2cc72952ac56.png\">\n",
    "\n",
    "When a GAN goes from noise $$ G: z \\rightarrow y $$, a conditional GAN goes from noise and input $$ G: \\{x,y\\} \\rightarrow y $$.\n",
    "\n",
    "In Pix2Pix paper, the objective function is modified. They add a regularization term: the L1 distance for the generator (how far the generated image is from the original one). They use L1 since L2 (euclidean distance) tends to tell to average all the pixel values to minimize it, hence encourage blurring\n",
    "of the images.\n",
    "\n",
    "$$ Loss_{L_1}(G) = E{[\\mid \\mid y-G(x,z)\\mid \\mid_1]} $$\n",
    "\n",
    "This term is added into the final objective with a scaling factor $$ \\alpha $$:\n",
    "\n",
    "$$ G^* = arg min_G max_D V(G,D) + \\alpha Loss_{L_1} (G) $$\n",
    "\n",
    "## Architecture\n",
    "\n",
    "In the original model, they don't use noise, only dropout. The reason is that in their experiments, it doesn't depend on the initial random point of the latent space. For both the generator and discriminator, they use modules of Convolution-BatchNorm-ReLU. They also utilize skip connection in their model of the generator. A skip connection simply means that we concanate everything at layer i to those at layer j. Like in the UNET architecture with encoder and decoder to reduce the information and then expand it, they use symetric layers to skip connections.\n",
    "\n",
    "Since L1 is to force low-frequency correctness (pixels that locate on the corresponding positions should look like each other), this blurs the image. The authors use a technique to enforce high frequencies (to return a crisp image), that only look at the structure in local patches. This is called PatchGAN. This term only penalizes structure at level of a patch. So the discriminator only try to classify if averagely speaking, all the patches in the images are real. This idea is like a Markov random field: assuming that pixels outside their patch are independent from them. Small patching still contribute to high quality results, and reduces the computation. \n",
    "\n",
    "Some other minor notes, they try to maximize the rate of discriminator instead of generator. But they also divide by two that rate so that the discriminator learns slowly. They use minibatch SGD with Adam solver, learning rate of 0.0002. One difficulty with evaluating synthetic images is that our usual euclidean distance doesn't really work. Since the mean square only measure the total distance, it doesn't capture the spatial concept (images are in 2 dimensions and each pixel location has its imporance).\n",
    "\n",
    "Here is the discriminator:\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2019/05/Plot-of-the-PatchGAN-Model-Used-in-the-Pix2Pix-GAN-Architecture.png\">\n",
    "\n",
    "Source: https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/\n",
    "\n",
    "The generator, with encoder and decoder blocks:\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2019/05/Plot-of-the-U-Net-Encoder-Decoder-Model-Used-in-the-Pix2Pix-GAN-Architecture-768x3679.png\">\n",
    "\n",
    "Source: https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d45649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of defining a composite model for training the generator model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# source image input\n",
    "\tin_src_image = Input(shape=image_shape)\n",
    "\t# target image input\n",
    "\tin_target_image = Input(shape=image_shape)\n",
    "\t# concatenate images channel-wise\n",
    "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
    "\t# C64\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# patch output\n",
    "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\tpatch_out = Activation('sigmoid')(d)\n",
    "\t# define model\n",
    "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "\treturn model\n",
    "\n",
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add downsampling layer\n",
    "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# conditionally add batch normalization\n",
    "\tif batchnorm:\n",
    "\t\tg = BatchNormalization()(g, training=True)\n",
    "\t# leaky relu activation\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\treturn g\n",
    "\n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add upsampling layer\n",
    "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# add batch normalization\n",
    "\tg = BatchNormalization()(g, training=True)\n",
    "\t# conditionally add dropout\n",
    "\tif dropout:\n",
    "\t\tg = Dropout(0.5)(g, training=True)\n",
    "\t# merge with skip connection\n",
    "\tg = Concatenate()([g, skip_in])\n",
    "\t# relu activation\n",
    "\tg = Activation('relu')(g)\n",
    "\treturn g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape=(256,256,3)):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n",
    "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "\te2 = define_encoder_block(e1, 128)\n",
    "\te3 = define_encoder_block(e2, 256)\n",
    "\te4 = define_encoder_block(e3, 512)\n",
    "\te5 = define_encoder_block(e4, 512)\n",
    "\te6 = define_encoder_block(e5, 512)\n",
    "\te7 = define_encoder_block(e6, 512)\n",
    "\t# bottleneck, no batch norm and relu\n",
    "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
    "\tb = Activation('relu')(b)\n",
    "\t# decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "\td1 = decoder_block(b, e7, 512)\n",
    "\td2 = decoder_block(d1, e6, 512)\n",
    "\td3 = decoder_block(d2, e5, 512)\n",
    "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "\t# output\n",
    "\tg = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "\tout_image = Activation('tanh')(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image)\n",
    "\treturn model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tfor layer in d_model.layers:\n",
    "\t\tif not isinstance(layer, BatchNormalization):\n",
    "\t\t\tlayer.trainable = False\n",
    "\t# define the source image\n",
    "\tin_src = Input(shape=image_shape)\n",
    "\t# connect the source image to the generator input\n",
    "\tgen_out = g_model(in_src)\n",
    "\t# connect the source input and generator output to the discriminator input\n",
    "\tdis_out = d_model([in_src, gen_out])\n",
    "\t# src image as input, generated image and classification output\n",
    "\tmodel = Model(in_src, [dis_out, gen_out])\n",
    "\t# compile model\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "\treturn model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
