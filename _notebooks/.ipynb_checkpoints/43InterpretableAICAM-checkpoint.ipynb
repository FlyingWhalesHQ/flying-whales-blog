{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c57e6c",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Interpretable AI: CAM\"\n",
    "date:   2023-06-07 10:14:54 +0700\n",
    "categories: MachineLearning\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "CAM (Class Activation Mapping) is a way to understand how and why a deep learning model has arrived at its prediction. It adds transparency to the applications, especially helpful in healthcare, finance, and autonomous vehicles where safety, compliance and robustness are important. It does so by visualize the decision of a convolutional neural network on the image. Roughly speaking, it shows where the model was looking at while it made the decision. This means that CAM provides a spatial map of the important features/pixels for the task at hand, giving some explanation for it. It can easily imagined that a heatmap of where the decision was focused on would provide great aid to doctors in medical imaging tasks.\n",
    "\n",
    "# CAM\n",
    "\n",
    "CAM's authors argue that the convolutional units in the CNN are the part that actually localize objects in the images despite having not being instructed explicitly. This ability would be diluted in the last layer of fully connected neurons. To avoid this, some new network architecture was developed to be fully convolutional. Of those, some use a global average pooling layer, that acts as a structural regularizer, preventing overfitting. The authors provide some tweaking to make such network able to retain the ability to localize discriminative regions.\n",
    "\n",
    "The authors use a similar network architecture to GoogLeNet, with mostly convolutional layers, and then just before the final softmax output, they put a global average pooling (GAP) and a fully connected layer. They would then project the weights of the fully connected layer back to the last convolutional feature maps (so it is called class activation mapping).\n",
    "\n",
    "<img width=\"1217\" alt=\"Screenshot 2023-06-07 at 14 14 26\" src=\"https://github.com/FlyingWhalesHQ/flying-whales-blog/assets/7457301/2c55180f-428a-419d-8b79-666dbd3b8a0b\">\n",
    "\n",
    "The authors also make a different between using global average pooling and global max pooling method. The global average pooling layer would encourage the network to recognize the whole extent of objects. Meanwhile the average max pooling only identify one discriminative part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ecc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Load VGG16 model\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "# Load the image\n",
    "img_path = '/kaggle/input/photo2monet-examples/cat-dog.jpg'  # Insert your image path here\n",
    "original_img = cv2.imread(img_path)\n",
    "width, height, _ = original_img.shape\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "# Get the predictions\n",
    "preds = model.predict(x)\n",
    "\n",
    "# Take the topmost class index\n",
    "top = np.argmax(preds[0])\n",
    "\n",
    "# Take output from the final convolutional layer\n",
    "output = model.output[:, top]\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "\n",
    "# Compute the gradient of the class output value with respect to the feature map\n",
    "grads = K.gradients(output, last_conv_layer.output)[0]\n",
    "\n",
    "# Pool the gradients over all the axes leaving out the channel dimension\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "# Weigh the output feature map with the computed gradient values\n",
    "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "\n",
    "for i in range(512):   # we have 512 features in our last conv layer\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "# Average the weighted feature map along the channel dimension resulting in a heat map of size 14x14 \n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "\n",
    "# Normalize the heat map to make the values between 0 and 1\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "\n",
    "# Resize heatmap to original image size\n",
    "heatmap = cv2.resize(heatmap, (height, width))\n",
    "\n",
    "# Plot original image and heatmap side by side\n",
    "plt.figure()\n",
    "plt.imshow(original_img)\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(heatmap)\n",
    "plt.title('Class Activation Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02faea84",
   "metadata": {},
   "source": [
    "![cat-dog](https://github.com/FlyingWhalesHQ/flying-whales-blog/assets/7457301/473b5b41-c0a4-4361-922d-fb27b6263f10)\n",
    "\n",
    "![CAM](https://github.com/FlyingWhalesHQ/flying-whales-blog/assets/7457301/eae97d23-ac18-487d-b23d-90e8bbfd3015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69ef24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
