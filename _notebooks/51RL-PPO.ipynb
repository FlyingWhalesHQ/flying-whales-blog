{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf833fe7",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"RL: Proximal Policy Optimization (PPO)\"\n",
    "date:   2023-06-20 10:14:54 +0700\n",
    "categories: DeepLearning\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Proximal Policy Optimization (PPO) belongs to the class of policy gradient methods for reinforcement learning that we have learned about in the previous post. PPO is developed by OpenAI, is effective and efficient, and is popular in the machine learning community.\n",
    "\n",
    "Let's recap a bit. Policy gradient methods aim to optimize the policy directly using the guidance from the gradient of the expected return. There are issues of those methods such as high variance, slow convergence, that PPO wants to address by using a simple idea: instead of making large update to the policy, make small update from the current policy (proximal update). Specifically, PPO uses a clipping function that introduces a lower and upper bound to the change that can be made to the policy in each update. The objective function incorporates this clip so that if the new policy deviates too much from the old one, there will be a penalty (a loss). By doing this, the difference between the new and old policy would be minimized to a specific range. \n",
    "\n",
    "# The Clip function\n",
    "\n",
    "$$ L_{CLIP}(\\theta) = E_t {[min(r_t(\\theta)) A_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t)]} $$\n",
    "\n",
    "with $$ \\theta $$ being the policy parameter, $$ E_t $$ denotes the empricial expectation. $$ r_t $$ is the ratio of the probability under the new and old policies. $$ A_t $$ is the estimated advantage at time t, $$ \\epsilon $$ is a hyperparameter, usually 0.1 or 0.2. \n",
    "\n",
    "\n",
    "In some tasks, PPO almost matches ACER (actor critic with experience replay), a far more complex method.\n",
    "\n",
    "In conclusion, PPO offers a balance between sample complexity, ease of implementation, computational cost, and performance which makes it a popular choice for many reinforcement learning tasks. Like all reinforcement learning algorithms, it's not a silver bullet and may not work well for all types of problems, but it has been demonstrated to be very effective in a wide range of applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065fc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
