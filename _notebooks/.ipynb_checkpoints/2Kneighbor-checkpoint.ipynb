{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e820e9",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"K neareast neighbors\"\n",
    "date:   2023-04-06 10:14:54 +0700\n",
    "categories: MachineLearning\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "K nearest neighbor is a simple algorithm in which the machine simply store the dataset (memorize it). $$ (x^{(i)}, y^{(i)}) \\in D (\\mid D \\mid = n) $$\n",
    "\n",
    "When a new data point comes in, it calculate the (could be weighted) distance to the closest data points (hence the name nearest neighbors) and assign a value. Since the process of training data is delayed til inference, it is call a lazy learning algorithm. Plus that the algorithm is to compare the new data point with current data points in the training set, it is called an instance based method. Further, without a functional form of the prediction phase, this is a non parametric model. For the classification task, it will assign the most voted label to the new point. For a regression task, it will take the average of the neighbors to be the prediction for that point. Those neighbors' values can be treated equally (uniformly) or weighted. More neighbors considered, smoother the decision boundary would become. \n",
    "\n",
    "The simpliticy, together with its lack of assumptions about the underlying data distribution are KNN advantanges.\n",
    "However, it can be computationally expensive to find the nearest neighbors in high-dimensional feature spaces and can be sensitive to the choice of distance metric leading to performance deteroriates.\n",
    "\n",
    "# Classification\n",
    "The word majority vote assumes a voting threshold of bigger than 50%, but in case of multiple classes, a three classe setting for example, you might just need as much as 34% to be the highest vote. So it is actually a plurality vote.\n",
    "\n",
    "Let's say we have c classes and a function to turn data point into label: $$ f(x) = y \\in \\{1,...,c\\} $$. When we have a query point x, we find its k nearest neighbors:\n",
    "\n",
    "$$ D_k = \\{(x^1, f(x^1)), ...(x^k, f(x^k))\\} $$\n",
    "\n",
    "KNN assumes that similar points tend to be near each other and the default distance to decide neighbor status is the Euclidean distance (or a $$ L_2 $$ distance) between 2 points is:\n",
    "\n",
    "$$ d(x^1, x^2) = \\sqrt{\\sum_{j=1}^{m} (x_{j}^{1} - x_{j}^{2})^2} $$\n",
    "\n",
    "Then the KNN hypothesis is to find x a label so that it maximizes a Krocker delta function:\n",
    "\n",
    "$$ h(x) = arg max_{y \\in \\{1,...,c\\}} \\sum_{i=1}^{k} \\delta(y, f(x^i)) $$\n",
    "\n",
    "with $$ \\delta(a,b) = \n",
    "\\begin{cases}\n",
    "        1 & \\text{if a = b}, \\\\\n",
    "        0 & \\text{if a $\\not =$ b}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The above function is equivalent to getting the mode (most frequent) value of the k neighbors:\n",
    "\n",
    "$$ h(x) = mode(\\{ y^1, y^2,..y^k \\}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b1822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
