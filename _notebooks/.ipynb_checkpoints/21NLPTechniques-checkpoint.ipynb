{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d447fb1d",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"NLP Techniques\"\n",
    "date:   2023-04-12 10:14:54 +0700\n",
    "categories: MachineLearning\n",
    "---\n",
    "# TOC\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Libraries](#lib)\n",
    "- [Preprocessing](#prep)\n",
    "- [Analysis](#anal)\n",
    "- [Task](#task)\n",
    "\n",
    "# Introduction\n",
    "Natural Language Processing (NLP) is an exciting and rapidly growing field of study that combines linguistics, computer science, and artificial intelligence. With the increasing availability of large amounts of text data and the growing need for intelligent communication with machines, NLP has become more important than ever. In this blog post, we will explore some recent academic papers on NLP and the advancements that they have made in the field. We will discuss topics such as language modeling, sentiment analysis, machine translation, and more. \n",
    "\n",
    "# Libraries\n",
    "Developing NLP applications can be challenging and time-consuming task without the right tools and resources. This is where NLP libraries come in, providing pre-built functions and modules that simplify the development process. In this article, we will explore some of the most popular NLP libraries and their features, highlighting their benefits and use cases.\n",
    "\n",
    "## nltk\n",
    "NLTK (Natural Language Toolkit) is a powerful open-source Python library that provides easy-to-use interfaces to numerous natural language processing (NLP) tasks, such as tokenization, part-of-speech tagging, parsing, sentiment analysis, and more.\n",
    "\n",
    "The library was initially developed at the University of Pennsylvania in the late 1990s and has since become one of the most popular and widely used NLP libraries in academia and industry. NLTK has been instrumental in democratizing access to NLP tools and techniques, making it possible for researchers, developers, and hobbyists to experiment with and build NLP applications with ease.\n",
    "\n",
    "## spaCy\n",
    "spaCy is an open-source software library for advanced natural language processing (NLP) in Python. It was developed with the goal of making NLP faster and more efficient while maintaining accuracy and ease of use. spaCy is designed to help developers build NLP applications with pre-trained models for named entity recognition, part-of-speech tagging, dependency parsing, and more. It also allows developers to train their own custom models to suit specific use cases.\n",
    "\n",
    "One of the standout features of spaCy is its speed. It was designed to process large volumes of text quickly, making it well-suited for tasks like web scraping, data mining, and information extraction. It accomplishes this speed by implementing several optimizations such as Cython integration, hash-based lookups, and multithreading.\n",
    "\n",
    "spaCy is also highly customizable and extensible. Developers can train their own models using spaCy's machine learning framework or integrate other third-party libraries into their workflows. Additionally, it offers a wide range of language support, including English, German, Spanish, Portuguese, Italian, French, Dutch, and others.\n",
    "\n",
    "\n",
    "## genism\n",
    "Gensim is a popular open-source library for natural language processing (NLP) tasks, including topic modeling, document similarity analysis, and word vector representations. It was developed by Radim Řehůřek in 2008 and is written in Python. Gensim is designed to handle large amounts of text data and provides efficient tools for working with text corpora.\n",
    "\n",
    "One of the key features of Gensim is its ability to perform topic modeling, which is the process of identifying the main themes or topics in a set of documents. Gensim provides several implementations of topic models, including Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), and Hierarchical Dirichlet Process (HDP).\n",
    "\n",
    "In addition to topic modeling, Gensim also provides tools for creating word vector representations, which are numerical representations of words that capture their meaning and context. These word vectors can be used for a variety of NLP tasks, such as text classification, sentiment analysis, and named entity recognition.\n",
    "\n",
    "## transformers\n",
    "The Transformer library is an open-source library for natural language processing (NLP) tasks that uses the Transformer model architecture, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. (2017). The Transformer model has become a popular choice for NLP tasks due to its ability to handle long-range dependencies and its parallelizability.\n",
    "\n",
    "The Transformer library is built on top of PyTorch and provides pre-trained models for a wide range of NLP tasks, such as classification, question-answering, and text generation. It also allows for fine-tuning pre-trained models on specific downstream tasks with just a few lines of code.\n",
    "\n",
    "In addition to pre-trained models, the Transformer library also provides a range of utilities for NLP tasks, such as tokenization, data processing, and evaluation metrics. It also supports multi-GPU training and inference, making it easy to scale up models to handle large datasets and complex tasks.\n",
    "\n",
    "Overall, the Transformer library is a powerful tool for NLP tasks that provides access to state-of-the-art pre-trained models and utilities, as well as the flexibility to fine-tune models for specific tasks.\n",
    "\n",
    "# Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "raw_text= 'Amazon Alexa, also known simply as Alexa, is a virtual assistant AI technology developed by Amazon, first used in the Amazon Echo smart speakers developed by Amazon Lab126. It is capable of voice interaction, music playback, making to-do lists, setting alarms, streaming podcasts, playing audiobooks, and providing weather, traffic, sports, and other real-time information, such as news. Alexa can also control several smart devices using itself as a home automation system. Users are able to extend the Alexa capabilities by installing \"skills\" additional functionality developed by third-party vendors, in other settings more commonly called apps such as weather programs and audio features.Most devices with Alexa allow users to activate the device using a wake-word (such as Alexa or Amazon); other devices (such as the Amazon mobile app on iOS or Android and Amazon Dash Wand) require the user to push a button to activate Alexa listening mode, although, some phones also allow a user to say a command, such as \"Alexa\" or \"Alexa wake\". Currently, interaction and communication with Alexa are available only in English, German, French, Italian, Spanish, Portuguese, Japanese, and Hindi. In Canada, Alexa is available in English and French with the Quebec accent).(truncated...).'\n",
    "text_doc=nlp(raw_text)\n",
    "\n",
    "# Tokenization\n",
    "token_count=0\n",
    "for token in text_doc:\n",
    "    print(token.text)\n",
    "    token_count+=1    \n",
    "print('No of tokens originally',token_count)\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "list_stopwords=list(stopwords)\n",
    "for word in list_stopwords[:7]:\n",
    "    print(word)\n",
    "\n",
    "token_count_without_stopwords=0\n",
    "filtered_text= [token for token in text_doc if not token.is_stop]\n",
    "for token in filtered_text:\n",
    "    token_count_without_stopwords+=1    \n",
    "print('No of tokens after removing stopwords', token_count_without_stopwords)\n",
    "\n",
    "# Remove punctuations\n",
    "filtered_text=[token for token in filtered_text if not token.is_punct]\n",
    "token_count_without_stop_and_punct=0\n",
    "for token in filtered_text:\n",
    "    print(token)\n",
    "    token_count_without_stop_and_punct += 1    \n",
    "print('No of tokens after removing stopwords and punctuations', token_count_without_stop_and_punct)\n",
    "    \n",
    "# Lemmatize\n",
    "lemmatized_list = [token.lemma_ for token in filtered_text]\n",
    "lemmatized = \" \".join(lemmatized_list)\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d545289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "data='It is my birthday today. I could not have a birthday party. I felt sad'\n",
    "data_doc=nlp(data)\n",
    "\n",
    "list_of_tokens=[token.text for token in data_doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Word frequency\n",
    "token_frequency=Counter(lemmatized_list)\n",
    "print(token_frequency)\n",
    "\n",
    "most_frequent_tokens=token_frequency.most_common(6)\n",
    "print(most_frequent_tokens)\n",
    "\n",
    "for token in filtered_text:\n",
    "    print(token.text.ljust(10),'-----',token.pos_, '----', token.lemma_, '---', ps.stem(token.text))\n",
    "\n",
    "## Part of speech tagging (POS)\n",
    "all_tags = {token.pos: token.pos_ for token in filtered_text}\n",
    "print(all_tags)\n",
    "\n",
    "nouns=[]\n",
    "verbs=[]\n",
    "\n",
    "for token in filtered_text:\n",
    "    if token.pos_ =='NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ =='VERB':\n",
    "        verbs.append(token)\n",
    "\n",
    "print('List of Nouns in the text\\n',nouns)\n",
    "print('List of verbs in the text\\n',verbs)\n",
    "\n",
    "## Remove junk POS\n",
    "numbers=[]\n",
    "for token in filtered_text:\n",
    "    if token.pos_=='X':\n",
    "        print(token.text)\n",
    "junk_pos=['X','SCONJ']\n",
    "def remove_pos(word):\n",
    "    flag=False\n",
    "    if word.pos_ in junk_pos:\n",
    "        flag=True\n",
    "    return flag\n",
    "\n",
    "revised_robot_doc=[token for token in filtered_text if remove_pos(token)==False]\n",
    "\n",
    "all_tags = {token.pos: token.pos_ for token in revised_robot_doc}\n",
    "print(all_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45888b0e",
   "metadata": {},
   "source": [
    "Counter({'Alexa': 11, 'Amazon': 7, 'device': 4, 'user': 4, 'develop': 3, 'smart': 2, 'interaction': 2, 'weather': 2, 'app': 2, 'allow': 2, 'activate': 2, 'wake': 2, 'available': 2, 'English': 2, 'know': 1, 'simply': 1, ...})\n",
    "\n",
    "[('Alexa', 11), ('Amazon', 7), ('device', 4), ('user', 4), ('develop', 3), ('smart', 2)]\n",
    "\n",
    "Amazon     ----- PROPN ---- Amazon --- amazon\n",
    "\n",
    "Alexa      ----- PROPN ---- Alexa --- alexa\n",
    "\n",
    "known      ----- VERB ---- know --- known\n",
    "\n",
    "simply     ----- ADV ---- simply --- simpli\n",
    "\n",
    "Alexa      ----- PROPN ---- Alexa --- alexa\n",
    "\n",
    "virtual    ----- ADJ ---- virtual --- virtual\n",
    "\n",
    "assistant  ----- NOUN ---- assistant --- assist\n",
    "\n",
    "AI         ----- PROPN ---- AI --- ai\n",
    "\n",
    "technology ----- NOUN ---- technology --- technolog\n",
    "\n",
    "developed  ----- VERB ---- develop --- develop\n",
    "\n",
    "Amazon     ----- PROPN ---- Amazon --- amazon\n",
    "\n",
    "Amazon     ----- PROPN ---- Amazon --- amazon\n",
    "\n",
    "Echo       ----- PROPN ---- Echo --- echo\n",
    "\n",
    "smart      ----- ADJ ---- smart --- smart\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d48c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word dependency\n",
    "for token in filtered_text:\n",
    "    print(token.text,'---',token.dep_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225e939",
   "metadata": {},
   "source": [
    "Amazon --- compound\n",
    "\n",
    "Alexa --- nsubj\n",
    "\n",
    "known --- acl\n",
    "\n",
    "simply --- advmod\n",
    "\n",
    "Alexa --- pobj\n",
    "\n",
    "virtual --- amod\n",
    "\n",
    "assistant --- compound\n",
    "\n",
    "AI --- compound\n",
    "\n",
    "technology --- attr\n",
    "\n",
    "developed --- acl\n",
    "\n",
    "Amazon --- pobj\n",
    "\n",
    "Amazon --- nmod\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f60d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(text_doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebae7b",
   "metadata": {},
   "source": [
    "<img width=\"1287\" alt=\"Screen Shot 2023-04-12 at 21 36 04\" src=\"https://user-images.githubusercontent.com/7457301/231491986-d66acb79-182a-4f73-b9c9-2d28e014da8d.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ad968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in filtered_text:\n",
    "    print(token.text,'---',token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8771ab",
   "metadata": {},
   "source": [
    "Amazon --- Alexa\n",
    "\n",
    "Alexa --- is\n",
    "\n",
    "known --- Alexa\n",
    "\n",
    "simply --- as\n",
    "\n",
    "Alexa --- as\n",
    "\n",
    "virtual --- assistant\n",
    "\n",
    "assistant --- technology\n",
    "\n",
    "AI --- technology\n",
    "\n",
    "technology --- is\n",
    "\n",
    "developed --- technology\n",
    "\n",
    "Amazon --- by\n",
    "\n",
    "Amazon --- Echo\n",
    "\n",
    "Echo --- speakers\n",
    "\n",
    "smart --- speakers\n",
    "\n",
    "speakers --- in\n",
    "\n",
    "developed --- speakers\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ef315",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=list(text_doc.sents)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609ebb1",
   "metadata": {},
   "source": [
    "is\n",
    "\n",
    "is\n",
    "\n",
    "control\n",
    "\n",
    "are\n",
    "\n",
    "require\n",
    "\n",
    "are\n",
    "\n",
    "is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2868aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named entity recognition (NER)\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "raw_text= 'Amazon Alexa, also known simply as Alexa, is a virtual assistant AI technology developed by Amazon, first used in the Amazon Echo smart speakers developed by Amazon Lab126.'\n",
    "\n",
    "tokens=word_tokenize(raw_text)\n",
    "tokens_pos=pos_tag(tokens)\n",
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b19772",
   "metadata": {},
   "source": [
    "[('Amazon', 'NNP'), ('Alexa', 'NNP'), (',', ','), ('also', 'RB'), ('known', 'VBN'), ('simply', 'RB'), ('as', 'IN'), ('Alexa', 'NNP'), (',', ','), ('is', 'VBZ'), ('a', 'DT'), ('virtual', 'JJ'), ('assistant', 'NN'), ('AI', 'NNP'), ('technology', 'NN'), ('developed', 'VBN'), ('by', 'IN'), ('Amazon', 'NNP'), (',', ','), ('first', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('Amazon', 'NNP'), ('Echo', 'NNP'), ('smart', 'JJ'), ('speakers', 'NNS'), ('developed', 'VBN'), ('by', 'IN'), ('Amazon', 'NNP'), ('Lab126', 'NNP'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8886ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "named_entity=ne_chunk(tokens_pos)\n",
    "print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc3f359",
   "metadata": {},
   "source": [
    "(S\n",
    "  (PERSON Amazon/NNP)\n",
    "  (GPE Alexa/NNP)\n",
    "  ,/,\n",
    "  also/RB\n",
    "  known/VBN\n",
    "  simply/RB\n",
    "  as/IN\n",
    "  (PERSON Alexa/NNP)\n",
    "  ,/,\n",
    "  is/VBZ\n",
    "  a/DT\n",
    "  virtual/JJ\n",
    "  assistant/NN\n",
    "  AI/NNP\n",
    "  technology/NN\n",
    "  developed/VBN\n",
    "  by/IN\n",
    "  (PERSON Amazon/NNP)\n",
    "  ,/,\n",
    "  first/RB\n",
    "  used/VBN\n",
    "  in/IN\n",
    "  the/DT\n",
    "  (ORGANIZATION Amazon/NNP Echo/NNP)\n",
    "  smart/JJ\n",
    "  speakers/NNS\n",
    "  developed/VBN\n",
    "  by/IN\n",
    "  (PERSON Amazon/NNP Lab126/NNP)\n",
    "  ./.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### spacy\n",
    "for entity in text_doc.ents:\n",
    "  print(entity.text,'--',entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079f7da",
   "metadata": {},
   "source": [
    "Amazon Alexa -- ORG\n",
    "\n",
    "Alexa -- ORG\n",
    "\n",
    "AI -- ORG\n",
    "\n",
    "Amazon -- ORG\n",
    "\n",
    "Amazon Echo -- ORG\n",
    "\n",
    "Amazon Lab126 -- ORG\n",
    "\n",
    "Alexa -- ORG\n",
    "\n",
    "Alexa -- ORG\n",
    "\n",
    "third -- ORDINAL\n",
    "\n",
    "Alexa -- ORG\n",
    "\n",
    "Alexa -- ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a90605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(text_doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825040e",
   "metadata": {},
   "source": [
    "<img width=\"834\" alt=\"Screen Shot 2023-04-12 at 21 43 22\" src=\"https://user-images.githubusercontent.com/7457301/231493972-d785fe7c-2b6f-404e-b5a9-8a201e222397.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd97e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out list of tagged organizations\n",
    "list_of_org=[]\n",
    "\n",
    "for token in filtered_text:\n",
    "  if token.ent_type_=='ORG':\n",
    "    list_of_org.append(token.text)\n",
    "\n",
    "print(list_of_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d8afc",
   "metadata": {},
   "source": [
    "['Amazon', 'Alexa', 'Alexa', 'AI', 'Amazon', 'Amazon', 'Echo', 'Amazon', 'Lab126', 'Alexa'...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f922053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK\n",
    "## Extractive summary is when the machine reuses the sentence and phrases\n",
    "## in the text to summarize it\n",
    "\n",
    "import gensim\n",
    "from gensim.summarization import summarize\n",
    "article_text='''Artificial Intelligence (AI) is a sub-field of computer science focused on creating intelligent programs that can perform tasks generally done by humans, including perception, learning, reasoning, pattern recognition, and decision-making. The term AI covers machine learning, predictive analytics, natural language processing, and robotics. AI is already part of our everyday lives, but its opportunities come with challenges for society and the law. There are different types of AI, including narrow AI (programmed to be competent in one specific area), artificial general intelligence (tasks across multiple fields), and artificial superintelligence (AI that exceeds human levels of intelligence).\n",
    "In general, Artificial Intelligence (AI) develops so rapidly and it has the potential to revolutionize the way we live and work. However, along with the tremendous benefits come significant ethical concerns. AI algorithms are trained on large amounts of data, and if that data is biased or flawed, it can lead to biased outcomes that disproportionately affect certain groups of people. Additionally, the increasing use of AI systems raises questions about privacy and security, as these systems collect vast amounts of personal data, with or without consent.\n",
    "To understand more on this, we will explore the ethical implications of AI, including the impact of bias, privacy, and security concerns. We will also discuss potential solutions to these issues and the importance of ethical considerations in the development and deployment of AI systems. To see a curated list of scary AI, visit aweful-ai.'''\n",
    "short_summary=summarize(article_text)\n",
    "print(short_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da89471",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) is a sub-field of computer science focused on creating intelligent programs that can perform tasks generally done by humans, including perception, learning, reasoning, pattern recognition, and decision-making.\n",
    "To understand more on this, we will explore the ethical implications of AI, including the impact of bias, privacy, and security concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_ratio=summarize(article_text,ratio=0.1)\n",
    "print(summary_by_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d4517",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) is a sub-field of computer science focused on creating intelligent programs that can perform tasks generally done by humans, including perception, learning, reasoning, pattern recognition, and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2badaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_word_count=summarize(article_text,word_count=30)\n",
    "print(summary_by_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c254762",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) is a sub-field of computer science focused on creating intelligent programs that can perform tasks generally done by humans, including perception, learning, reasoning, pattern recognition, and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### spacy\n",
    "## In spacy, we score keywords and key sentences\n",
    "## Then we print out the most important sentences, to summarize the text\n",
    "\n",
    "keywords_list = []\n",
    "\n",
    "desired_pos = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "for token in filtered_text: \n",
    "  if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "    continue\n",
    "  if(token.pos_ in desired_pos):\n",
    "    keywords_list.append(token.text)\n",
    "    \n",
    "from collections import Counter\n",
    "dictionary = Counter(keywords_list) \n",
    "print(dictionary)\n",
    "\n",
    "highest_frequency = Counter(keywords_list).most_common(1)[0][1] \n",
    "\n",
    "for word in dictionary:\n",
    "    dictionary[word] = (dictionary[word]/highest_frequency) \n",
    "print(dictionary)\n",
    "\n",
    "score={}\n",
    "\n",
    "for sentence in text_doc.sents: \n",
    "    for token in sentence:\n",
    "        if token.text in dictionary.keys():\n",
    "            if sentence in score.keys():\n",
    "                score[sentence]+=dictionary[token.text]\n",
    "            else:\n",
    "                score[sentence]=dictionary[token.text]\n",
    "print(score)\n",
    "\n",
    "sorted_score = sorted(score.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "text_summary=[]\n",
    "\n",
    "no_of_sentences=4\n",
    "\n",
    "total = 0\n",
    "for i in range(len(sorted_score)):\n",
    "    text_summary.append(str(sorted_score[i][0]).capitalize()) \n",
    "    total += 1\n",
    "    if(total >= no_of_sentences):\n",
    "        break \n",
    "\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5553add",
   "metadata": {},
   "source": [
    "['Most devices with alexa allow users to activate the device using a wake-word (such as alexa or amazon); other devices (such as the amazon mobile app on ios or android and amazon dash wand) require the user to push a button to activate alexa listening mode, although, some phones also allow a user to say a command, such as \"alexa\" or \"alexa wake\".', 'Amazon alexa, also known simply as alexa, is a virtual assistant ai technology developed by amazon, first used in the amazon echo smart speakers developed by amazon lab126.', 'Users are able to extend the alexa capabilities by installing \"skills\" additional functionality developed by third-party vendors, in other settings more commonly called apps such as weather programs and audio features.', 'Currently, interaction and communication with alexa are available only in english, german, french, italian, spanish, portuguese, japanese, and hindi.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generative summary\n",
    "## is when we generate original sentences and phrases to summarize the text\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "raw_text='''You can notice that in the extractive method, the sentences of the summary are all taken from the original text. There is no change in structure of any sentence.\n",
    "Generative text summarization methods overcome this shortcoming. The concept is based on capturing the meaning of the text and generating entitrely new sentences to best represent them in the summary.\n",
    "These are more advanced methods and are best for summarization. Here, I shall guide you on implementing generative text summarization using Hugging face .'''\n",
    "summarizer = pipeline(\"summarization\", model=\"stevhliu/my_awesome_billsum_model\", max_length=50)\n",
    "summarizer(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab3e89",
   "metadata": {},
   "source": [
    "[{'summary_text': 'Generative text summarization methods overcome this shortcoming. The concept is based on capturing the meaning of the text and generating entitrely new sentences to best represent them in the summary.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Language translation\n",
    "from transformers import pipeline\n",
    "translator_model=pipeline(task='translation_en_to_ro')\n",
    "translator_model\n",
    "translator_model('Hello, good morning, welcome to this country!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93e358",
   "metadata": {},
   "source": [
    "[{'translation_text': 'Salut, bună dimineaţă, bine aţi venit în această ţară!'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text generation\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2DoubleHeadsModel\n",
    "\n",
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model=GPT2DoubleHeadsModel.from_pretrained('gpt2-medium')\n",
    "my_text='Nowadays people are'\n",
    "ids=tokenizer.encode(my_text)\n",
    "ids\n",
    "my_tensor=torch.tensor([ids])\n",
    "model.eval()\n",
    "result=model(my_tensor)\n",
    "predictions=result[0]\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "predicted_index\n",
    "predicted_text = tokenizer.decode(ids + [predicted_index])\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf8449",
   "metadata": {},
   "source": [
    "Nowsaday people are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be751ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_words_to_generate=30\n",
    "\n",
    "text = 'Nowsaday people are not'\n",
    "\n",
    "for i in range(no_of_words_to_generate):\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "    result = model(input_tensor)\n",
    "    predictions = result[0]\n",
    "    predicted_index = torch.argmax(predictions[0,-1,:]).item()\n",
    "    text = tokenizer.decode(input_ids + [predicted_index])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58dfbbb",
   "metadata": {},
   "source": [
    "Nowsaday people are not the only ones who have been affected by the recent spate of violence. \"I've been in the area for a while now and I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f81298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chatbot\n",
    "\n",
    "def take_last_tokens(inputs, note_history, history):\n",
    "    \"\"\"Filter the last 128 tokens\"\"\"\n",
    "    if inputs['input_ids'].shape[1] > 128:\n",
    "        inputs['input_ids'] = torch.tensor([inputs['input_ids'][0][-128:].tolist()])\n",
    "        inputs['attention_mask'] = torch.tensor([inputs['attention_mask'][0][-128:].tolist()])\n",
    "        note_history = ['</s> <s>'.join(note_history[0].split('</s> <s>')[2:])]\n",
    "        history = history[1:]\n",
    "\n",
    "    return inputs, note_history, history\n",
    "\n",
    "\n",
    "def add_note_to_history(note, note_history):\n",
    "    \"\"\"Add a note to the historical information\"\"\"\n",
    "    note_history.append(note)\n",
    "    note_history = '</s> <s>'.join(note_history)\n",
    "    return [note_history]\n",
    "\n",
    "\n",
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    if history: \n",
    "        history_useful = ['</s> <s>'.join([str(a[0])+'</s> <s>'+str(a[1]) for a in history])]\n",
    "    else:\n",
    "        history_useful = []\n",
    "    \n",
    "    history_useful = add_note_to_history(message, history_useful)\n",
    "    inputs = tokenizer(history_useful, return_tensors=\"pt\")\n",
    "    inputs, history_useful, history = take_last_tokens(inputs, history_useful, history)\n",
    "    \n",
    "    reply_ids = model.generate(**inputs)\n",
    "    response = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
    "    history_useful = add_note_to_history(response, history_useful)\n",
    "    \n",
    "    list_history = history_useful[0].split('</s> <s>')\n",
    "    history.append((list_history[-2], list_history[-1]))\n",
    "    \n",
    "    return history, history\n",
    "\n",
    "chat(\"hello\",['how are you'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f84594",
   "metadata": {},
   "source": [
    "(['how are you',\n",
    "  ('hello',\n",
    "   ' Hi there, how are you? I just got back from a trip to the beach.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaa822",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question answering\n",
    "## In this task, the machine receives a paragraph with the information\n",
    "## it then extracts the words to answer the given question\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c806f4e",
   "metadata": {},
   "source": [
    "{'id': '56ce6f81aab44d1400b8878b',\n",
    " 'title': 'To_Kill_a_Mockingbird',\n",
    " 'context': 'Scholars have characterized To Kill a Mockingbird as both a Southern Gothic and coming-of-age or Bildungsroman novel. The grotesque and near-supernatural qualities of Boo Radley and his house, and the element of racial injustice involving Tom Robinson contribute to the aura of the Gothic in the novel. Lee used the term \"Gothic\" to describe the architecture of Maycomb\\'s courthouse and in regard to Dill\\'s exaggeratedly morbid performances as Boo Radley. Outsiders are also an important element of Southern Gothic texts and Scout and Jem\\'s questions about the hierarchy in the town cause scholars to compare the novel to Catcher in the Rye and Adventures of Huckleberry Finn. Despite challenging the town\\'s systems, Scout reveres Atticus as an authority above all others, because he believes that following one\\'s conscience is the highest priority, even when the result is social ostracism. However, scholars debate about the Southern Gothic classification, noting that Boo Radley is in fact human, protective, and benevolent. Furthermore, in addressing themes such as alcoholism, incest, rape, and racial violence, Lee wrote about her small town realistically rather than melodramatically. She portrays the problems of individual characters as universal underlying issues in every society.',\n",
    " 'question': 'What genre of book is To Kill a Mockingbird typically called?',\n",
    " 'answers': {'text': ['Southern Gothic and coming-of-age or Bildungsroman novel'],\n",
    "  'answer_start': [60]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70988d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text classification\n",
    "## In this example, we classify the sentiment of movie reviews\n",
    "## with a pretrained BERT\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_imdb[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_imdb[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "\n",
    "# model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n",
    "\n",
    "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"stevhliu/my_awesome_model\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124d373",
   "metadata": {},
   "source": [
    "[{'label': 'LABEL_1', 'score': 0.9994940757751465}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63260c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n",
    "logits = model(**inputs).logits\n",
    "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712cd80",
   "metadata": {},
   "source": [
    "'LABEL_1'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
