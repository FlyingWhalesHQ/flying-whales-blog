---
layout: post
title:  "Dense Neural Network"
date:   2023-03-08 10:14:54 +0700
categories: jekyll update
---

# TOC

- [Definition](#define)
- [Backpropagation](#backprop)
- [Gradient descent](#grad)
- [Code example](#code)


# Definition

Remember the linear combination of input x (note that x can be non linear):

$$ \hat{y}=h_{\theta}(x) = \theta \cdot x $$ 

Also remember when we wrap this linear combination in all kinds of non linear function (sigmoid, sign, softmax). There are some other non linear functions that are also as popular: tanh, ReLU.. In general, those transformations are called activation functions. They are there to transform the data flow and to make the investigation intesresting (instead of a big chunk of linear combination) for complex problems.

In deep learning, each of those nonlinear transformations is one neuron. Hence the perceptron has one neuron. Since it uses the sign function, we can call it a sign neuron. In general, the last neurons that output classes (using softmax) or values are called output layer. Those neurons between input and output layer are called hidden layers since they transform input and continue to do so before outputing some thing for classification or regression.

This kind of network that each neuron of one layer is connected (to be input) to all the neuron for the next layer is called a dense network, or a fully connected feedforward network. It is called feedforward (or sequential) since the input flows (and is transformed) one-way forward from the input to output layer.

## ReLU

ReLU, shorted for Rectified linear unit, is an incredibly fast and straightforward but successful activation function. It is:

$$ ReLU(x) = max(0, h_{\theta}) $$

ReLU returns either 0 or the linear combination of input, whichever is greater.

## A 2-layer neural network

Let's say we have 3 attributes $$ x_1, x_2, x_3 $$. The linear combination would be:

$$ \hat{y_1} = h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 $$

with $$ x_0 = 1 $$ and $$ \theta_0 $$ to be the bias. Take this through the transformation of ReLU and we have the first neuron of the first hidden (middle) layer:

$$ a_1 = ReLU(\theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3) $$

For the second neuron of the first hidden layers:

$$ a_2 = ReLU(\theta_4 x_0 + \theta_5 x_1 + \theta_6 x_2 + \theta_7 x_3) $$

For the output layer:

$$ h_\theta(x) = \theta_8 a_1 + \theta_9 a_2 $$

Usually we don't use activation for the output layer that predicts a value (regression). If we need nonnegative value we can use ReLU. For classification problem, we can use a softmax.

With this setup, we use a MSE for loss function of a regression problem and cross entropy for classification problem. To optimize loss function, we calculate gradient descent. Backpropagation is a technique to calculate gradient so we can use it for the descent step. In big overview, this whole process of training a neural network means:

- to randomly initialize the parameter vector

- use those starting parameters to do a forward calculation (multiply with input then transform) outputing prediction

- measure the error of prediction

- do a backward pass: calculate how much each parameter is responsible for the error

- with that in mind, update the parameters in the direction of descending the gradient so that we are on the way to the optimal loss

The backward pass is called backpropagation: we backward propagate the error.

## Backpropagation


# 



```python

```
