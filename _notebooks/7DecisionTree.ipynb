{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5ab79ab",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Decision Tree\"\n",
    "date:   2023-03-06 8:14:54 +0700\n",
    "categories: jekyll update\n",
    "---\n",
    "\n",
    "# TOC\n",
    "- [Definition](#define)\n",
    "- [ID3 vs CART](#algo)\n",
    "- [Entropy vs Gini vs MSE](#loss)\n",
    "- [Code example](#code)\n",
    "\n",
    "# Definition <a name=\"define\"></a>\n",
    "Decision tree is a white box model, in contrast to a black box model such as neural net. It is so because it is pretty straightforward and simple. In decision tree, there is a sequence of decisions to make, towards an end goal. For example, the end goal is that we want to know whether a person would be accepted into a school. We would study the problem and make questions: whether they have a technical educational background, do they like the school, what was their parents grade when they were at that school,etc. All the binary choice and multiple choice questions. And it also doesn't have to be just for classification question, it can predict value too, by averaging similar training instance. It is a tolerating model.\n",
    "\n",
    "<img src='#'>\n",
    "\n",
    "As we can see, the diagram looks like a tree, with the first node to be the root node, since it is the most important question to take into account in the problem. It only seems logical that we need the consider the most burning question first. Here the root node ask whether his (or her) math capability is sufficient. It might be a talent school that is difficult at its entrance exam. Of course this is a simplication but let's go with it for the sake of a stylized situation. After we divide the possibility with the threshold of 8/10 for his math capability, if he is lucky and ends up with a grade of 9, we need to see whether he actually likes the school or not. That \"like?\" node would be one of the child note of the root one. If he doesn't, he might as well choose another one to the interest of his own. What if he doesn't like it? Should it be the end of the problem? This is where we should be mindful. Maybe he is a grown-up and the choice of his life is up to him already. But it can also be that this school is at the beginning of his life and the influence of his parents are still significant and meaningful. So we would ask another question, hopefully this settles the matter, whether his parents were enrolled in this school in their time. If he doesn't like it, but his parents have their legacies, then the consensus could be that he would apply for that school anyway. In this case, the last node (also called the leaf node) says that it is highly likely is that he got accepted. That would be it for our prediction.\n",
    "\n",
    "Let's draw another diagram to demonstrate the intuition above in a visual hence even easier way:\n",
    "\n",
    "<img src='#>\n",
    "\n",
    "On x axis, we have the math grade. At the threshold of 8, the space is vertically divided into two sub space: grades bigger than 8 (say, 9 or 10), and grades smaller than 8 (from 0 to 7). On the y axis, there is the likeness that the student expresses toward the school. Let's encode anything larger than 0.5 to be yes, he likes and the rest to be no, he doesn't want. In the case he doesn't want the school despite the fact that he earns 9 over 10 in his current math class, we end up with the small subpace in the bottom right. Now the deciding question comes in and divides the subspace into two small subspace, this time for good, of whether his parents were in this prestigious school in their time or not. If they were, then the student migh unhappily joined since it is the choice in his best future interest, despite his state of emotions.\n",
    "\n",
    "This easy example demonstrates the simple beauty of this model. However, we are still in machine learning, this model has an algorithm and a loss function with all its complexness to optimize over, too. In the following sections we would get to know the algorithms and then the loss functions.\n",
    "\n",
    "# Algorithm: ID3 vs CART <a name=\"algo\"></a>\n",
    "\n",
    "As we can see from doing the above exercise, a good algorithm for decision tree needs to know what questions to ask. To know how good a question is, we can judge it based on how it divides the possibility space. One way that comes to our mind is that at each node, we just choose what is the best (also called a greedy strategy). This rules out nodes that provide little today but provide a lot tomorrow. For example, at node \"likeness\", if the student choose to follow his thoughts, it might be that he ends up with a future of less. In any cases, when study this decision tree approach, people have come up with a measurement of how good a dividing question is, by investigating how good it separates the data into classes. For example, the question of \"math > 8?\" contributes by separate the dataset into \"accepted\" or \"out\" easily. If you get less than 8, the odds are clearly not in your favor. In technical terms, we say that this node is pure. On the other hand, the node of the question \"like?\" divides but not all the dataset. In other words, it is an impure question. It is impure in the sense that when we ask that question, we don't have a clear or definite answer of whether he got accepted.\n",
    "\n",
    "# Loss function: Entropy vs Gini vs MSE <a name=\"loss\"></a>\n",
    "\n",
    "There are several ways to define the term purity technically. One of those is using entropy. Entropy concept comes from physics, to measure how orderly a system is. If everything is in order (i.e. neat) the entropy is low. Entropy then is used in information theory, to calculate information and noise in the signals. \n",
    "\n",
    "Applying this concept into decision tree, we have that a node is pure if its entropy is low. A node with lowest entropy is like the \"math grade > 8\" node, in which the requirement of the school to be least 8 points. That node divides the dataset into \"considering\" and \"out\" in a clean fashion: if you get less than 8, simply go another way. A node has maximum entropy when the dataset is divided equally (all outcomes become equal if we consider according to that node).\n",
    "\n",
    "Using entropy to measure purity, the loss function would be the total weighted entropy at all leaves (ending notes). Entropy is weighted with the number of data points in that node. To minimize this loss, at each node, we consider the way of division such that the entropy can decrease in the most possibile fashion. This is naturally a recursive algorithm: we start from the root, choose the best, then on to the next, choose the best, and so on.\n",
    "\n",
    "The decision tree diverges from linear combination algorithms. We consider N input data, X attributes, and the probability for each data point is p. Entropy of the distribution of p is:\n",
    "\n",
    "$$ H(p) = - \\sum_{i=1}^{n} p_{i} log(p_{i}) $$\n",
    "\n",
    "At each node, with N data points and K classes, the probability of class k is $$ \\fraction{N_{k}}{N} $$. Entropy is that node would be:\n",
    "\n",
    "$$ H = - \\sum_{k=1}^{K} \\frac{N_{k}}{N} log \\frac{N_{k}}{N} $$\n",
    "\n",
    "Now we calculate the influence of each attribute x. Entropy of attribute x would be the number of data points n having that attribute x in child node c:\n",
    "\n",
    "$$ H(x) = \\sum_{k=1}{K} \\frac{n}{N} H_{c} $$\n",
    "\n",
    "The amount of information gained using attribute x is:\n",
    "\n",
    "$$ G(x) = H - H(x) $$\n",
    "\n",
    "In ID3, to find the questions to ask is to find the next-importance attribute x. To do this, we try to maximize the information gain which is also minimize the entropy of that attribute:\n",
    "\n",
    "$$ x^{*} = argmax_{x} G(x) = arg min_{x} H(x) $$\n",
    "\n",
    "Come back to the example at the beginning of this post, let's collect a dataset of students who also consider joining that school, with those attributes: math-grade, likeness, parents-were-in ,etc. \n",
    "\n",
    "Math-grade can be from 0 to 10. Likeness is binary, i.e. can be yes or no, if we need to encode it, 0 being negative. Parents-were-in is also binary. Same for target \"accepted\".\n",
    "\n",
    "|id|math-grade|likeness|parents|accepted|\n",
    "|--|--|--|--|--|\n",
    "|1|8|yes|yes|no|\n",
    "|2|9|yes|yes|yes|\n",
    "|3|10|no|yes|no|\n",
    "|4|5|yes|yes|no|\n",
    "|5|10|no|yes|yes|\n",
    "\n",
    "With 5 data points, we have 2 \"accepted\" and 3 \"out\". The entropy of root node is:\n",
    "\n",
    "$$ H = - \\frac{2}{5} log \\frac{2}{5} - \\frac{3}{5} log \\frac {3}{5} \\approx 0.673  $$\n",
    "\n",
    "Consider math-grade attribute: it has 2 values of bigger-than-8 and the rest. Each of those value has one child node. Child node bigger-than-8 has 3 data points, the other has 2.\n",
    "\n",
    "|id|math-grade|likeness|parents|accepted|\n",
    "|--|--|--|--|--|\n",
    "|2|9|yes|yes|yes|\n",
    "|3|10|no|yes|no|\n",
    "|5|10|no|yes|yes|\n",
    "\n",
    "|id|math-grade|likeness|parents|accepted|\n",
    "|--|--|--|--|--|\n",
    "|1|8|yes|yes|no|\n",
    "|4|5|yes|yes|no|\n",
    "\n",
    "Child node with grade <= 8 has outcomes of both \"out\". So entrpy of math-grade <= 8 would be 0 (which means that if you achieve math grade of less than or equal to 8, you are sorted).\n",
    "\n",
    "Child node with grade > 8 have 3 data points. Of which one says no at the end and two say yes.\n",
    "\n",
    "$$ H(math_grade_{>8}) = - \\frac{1}{3} log \\frac{1}{3} - \\frac{2}{3} log \\frac {2}{3} \\approx 0.634 $$\n",
    "\n",
    "The entropy that this \"math-grade\" attribute contributes to the root node would be:\n",
    "\n",
    "$$ H_{math-grade} = \\frac{3}{5} H(math-grade_{>8}) + \\frac{2}{5} * H(math-grade_{<=8}) = \\frac{3}{5} * 0.634 + \\frac{2}{5} * 0 \\approx 0.380 $$\n",
    "\n",
    "Now we consider attributes likeness and parents-were-in. We then compare the entropies and choose the one that contribute least entropy to be the first node in the decision tree. The calculation for the rest is carried out recursively.\n",
    "\n",
    "For likeness:\n",
    "\n",
    "|id|math-grade|likeness|parents|accepted|\n",
    "|--|--|--|--|--|\n",
    "|1|8|yes|yes|no|\n",
    "|2|9|yes|yes|yes|\n",
    "|4|5|yes|yes|no|\n",
    "\n",
    "$$ H(likeness_{yes}) = - \\frac{1}{3} log \\frac{1}{3} - \\frac{2}{3} log \\frac{2}{3} \\approx 0.636 $$\n",
    "\n",
    "|id|math-grade|likeness|parents|accepted|\n",
    "|--|--|--|--|--|\n",
    "|3|10|no|yes|no|\n",
    "|5|10|no|yes|yes|\n",
    "\n",
    "$$ H(likeness_{no}) = - \\frac{1}{2} log \\frac{1}{2} - \\frac{1}{2} log \\frac{1}{2} \\approx 0.693 $$\n",
    "\n",
    "The total entroy of this attribute \"likeness\" is:\n",
    "\n",
    "$$ H(likeness) = \\frac{3}{5} * 0.636 + \\frac{2}{5} * 0.693 \\approx 0.659 > 0.380 = H(math-grade) $$\n",
    "\n",
    "Please calculate the contributing entropy of attribute parents-were-in and check whether attribute math-grade is the most important one.\n",
    "\n",
    "# Code example <a name=\"code\"></a>\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
