---
layout: post
title:  "t-SNE"
date:   2023-05-29 10:14:54 +0700
categories: MachineLearning
---

# TOC

- [Introduction](#intro)
- [Distance measurements](#mea)
    - [Cross entropy](#ent)
    - [Kullback–Leibler divergence](#kl)
    - [Jensen Shannon divergence](#js)
- [t-SNE](#tsne)
    - [Stochastic neighbor embedding](#sne)
    - [t-SNE](#tsne2)

# Introduction
t-SNE is for visualization of high dimensional data. 

Let's see some distance measurements between distributions and then the t-SNE.

# Distance measurements

## Cross entropy

Given a random variable X, entropy is a way to measure its uncertainty: $$ H(X) = \sum_{x \in \Omega} -p(x) log (p(x)) $$. It measures the amount of information in the random variable, usually in bits. 

The joint entropy of a pair of random variables is the amount of information needed to specify their values: $$ H(X,Y) = - \sum_{x\in X} \sum_{y\in Y} p(x,y) log (p(X,Y)) $$. 

Cross entropy is a measure of the difference between two probability distributions (p and q). It is the number of bits to encode data from distribution p when we use q. $$ H_p(p,q) = E_p {[-log(q(x))]} = - \sum_{x \in \Omega} p(x) log(q(x)) $$ 

Note that in information theory, if we use $$ log_2 $$ we have the number of bits of information, but if we use the natural logarithm, $$ log $$ or $$ ln $$, we are talking about another unit of information: the nat. 

## KL divergence
KL divergence, short for Kullback–Leibler divergence, $$ D_{KL}(p_X, p_Y) $$ measures how much the distribution defined by $$ p_Y(y) $$ is dissimilar from the reference distribution defined by $$ p_X(x) $$. Hence it measures the information lost when $$ p_Y(x) $$ is used to approximate $$ p_X(x) $$. It is also called relative entropy.

Let X and Y be two discrete random variables with probability mass function $$ p_X(x) $$ and $$ p_Y(y) $$. The KL divergence of $$ p_Y(y) $$ from $$ p_X(x) $$ is $$ D_{KL}(p_X,p_Y) = H(p_X,p_Y) - H(p_X) = - \sum_{x \in \Omega} p_X(x) log(p_Y(x)) - (- \sum_{x \in \Omega} p_X(x) log(p_X(x))) $$

$$ = \sum_{x \in \Omega} p_X(x) log(p_X(x)) - \sum_{x \in \Omega} p_X(x) log(p_Y(x)) = \sum p_X(x) log(\frac{p_X(x)}{p_Y(x)}) $$

For X and Y being two continuous random variables, the KL divergence of $$ f_Y(y) $$ from $$ f_X(x) $$ is $$ D_{KL} (f_X, f_Y) = - \int f_X(x) ln(\frac{f_y(x)}{f_X(x)})dx $$

The KL divergence has a meaning of being the expected number of extra bits to code samples from $$ p_X(x) $$ using code of $$ p_Y(x) $$, instead of $$ p_X(x) $$. Why would we want to do that? Presumably in the case that the true data distribution $$ p_X(x) $$ is not available and accessible to us, so we use a model ($$ p_Y(x) $$) to approximate it. 

The KL divergence is non negative: Let $$ p_X(x) $$ and $$ p_Y(y) $$ be two probability mass functions. When the two probability distribution coincide, that is $$ p_X(x) = p_Y(x) $$ then $$ D_{KL}(p_X,p_Y) = 0 $$. Otherwise, when they do not conincide, $$ D_{KL} (p_X,p_Y) > 0 $$. Firstly, know that if the two probability distribution are equal, then $$ \frac{p_Y(x)}{p_X(x)} = 1 $$. Substitute this into $$ D_{KL} = - \sum p_X(x) log (\frac{p_Y(x)}{p_X(x)}) = - \sum p_X(x) log(1) = - \sum p_X(x) . 0 = 0 $$.

If they are not equal, then $$ D_{KL} = - \sum p_X(x) log(\frac{p_Y(x)}{p_X(x)} ) = E {[ -log(\frac{p_Y(X)}{p_X(X)}) ]} $$ since this is the expectation of the probability distribution X. Using Jensen's inequality, $$ D_{KL} (p_X, p_Y) > -log(E {[\frac{p_Y(X)}{p_X(X)}]}) = - log(\sum p_X(x) \frac{p_Y(x)}{p_X(x)}) = -log(\sum p_Y(x)) \geq -log(1) $$ since the sum of probability is at most 1. $$ -log(1) = 0 $$ so $$ D_{KL} > 0 $$ 

### Basic example

Consider two distributions, red and blue. 


```python
import matplotlib.pyplot as plt
 
# creating the dataset
x = [0,1,2,3,4]
y = [0.2, 0.5, 0.1, 0.3, 0.4]
x2 = [0.4,1.4,2.4,3.4,4.4]
y2 = [0.3, 0.4, 0.1, 0.2, 0.3]

fig = plt.figure(figsize = (10, 5))

# creating the bar plot
plt.bar(x, y, color ='red', width = 0.4,label="dist1")
plt.bar(x2, y2, color ='blue', width = 0.4,label="dist2")
plt.legend()
plt.show()
```


    
![png](36t-SNE_files/36t-SNE_1_0.png)
    


Analytically, the KL divergence is calculated as follows:

$$ D_{KL}(p_X, p_Y) = \sum p_X(x) log(\frac{p_X(x)}{p_Y(x)}) $$

$$ = 0.2 log(\frac{0.2}{0.3}) + 0.5 log(\frac{0.5}{0.4}) + 0.1 log(\frac{0.1}{0.1}) + 0.3 log(\frac{0.3}{0.2}) + 0.4 log(\frac{0.4}{0.3}) = 0.267 $$

### Code example

Given two distributions, let's calculate the KL divergence of these two using code and using the provided function by scipy library in Python.


```python
dist1 = [0.2, 0.5, 0.1, 0.3, 0.4]
dist2 = [0.3, 0.4, 0.1, 0.2, 0.3]

import numpy as np
from scipy.special import rel_entr

def kl_divergence(d1, d2):
    return sum(d1[i] * np.log(d1[i]/d2[i]) for i in range(len(d1)))

print ("KL divergence (d1 || d1): {}".format(kl_divergence(dist1, dist1)))
print ("KL divergence (d1 || d2): {}".format(kl_divergence(dist1, dist2)))

print("--------------------")
print("With scipy rel_entr function")

print ("KL divergence (d1 || d1): {}".format(sum(rel_entr(dist1, dist1))))
print ("KL divergence (d1 || d2): {}".format(sum(rel_entr(dist1, dist2))))

```

    KL divergence (d1 || d1): 0.0
    KL divergence (d1 || d2): 0.2671911154486337
    --------------------
    With scipy rel_entr function
    KL divergence (d1 || d1): 0.0
    KL divergence (d1 || d2): 0.2671911154486337


Note that since KL divergence is not a symmetric indicator ( $$ D_{KL}(p_X,p_Y) \neq D_{KL}(p_Y, p_X) $$), it is not really a distance measurement. It measures the dissimilarity. KL divergence and cross entropy can have many application. For example, in model selection, a model with smaller KL divergence or cross entropy suggests a "closer" prediction to true labels hence we can choose the it. In NLP, we can compare document-topic distribution divergence and make accordingly actions. In some variational auto encoders, KL divergence is used as a part of the loss function. It measures the difference between the learned distribution and the prior, pushing the learned distribution toward the prior during training. KL divergence and cross entropy can also be used for information retrieval when we compare a query to documents in a database to identify the most relevant matches. They also have some applications in reinforcement learning, when they are used to limit the update in each learning step, so that the new policy stays close to the old policy, making learning stable.

## Jensen Shannon divergence

Jensen Shannon (JS) divergence is a weighted sum of KL divergence since it makes a symmetric indicatior out of KL:

$$ D_{JS} (p_X, p_Y) = \frac{1}{2} D_{KL} (p_X \mid \mid \frac{p_X + p_Y}{2}) + \frac{1}{2} D_{KL} (p_Y \mid \mid \frac{p_X + p_Y}{2}) $$

Since it takes the average of the two distributions and measure the KL difference between each distribution to the average, (and take average of that), it is also called total divergence to the average. The distribution after taking average of the two distributions is called the mixture probability distribution. 

<img src="https://miro.medium.com/v2/resize:fit:1400/0*FeaFgw2gjfHGs9fv">
Image: Getting a mixture distribution

For discrete variables, the mixture distribution is $$ \frac{p_X(x) + p_Y(x)}{2} $$ and the discrete form of JS divergence is:

$$ \frac{1}{2} \sum p_X(x) * log(\frac{p_X(x)}{mixture(x)}) + \frac{1}{2} \sum p_Y(x) * log(\frac{p_Y(x)}{mixture(x)}) $$

Some differences with the KL divergence include its symmetry and finite nature. The JS divergence is bounded by 0 and 1 if we use the base 2 logarithm, and it is bounded by 0 and log(2) if we use base e (natural logarithm). As above, if we use $$ log_2 $$ we result in bit and if we use $$ log_e $$ we result in nat. The squareroot of JS divergence is called JS distance.

JS divergence can be used for drift monitoring: to detect changes between training distribution and production distribution to retrain (adapt) the model with the change. It can also be used to make sure that input or output data in production doesn't change drammatically from a baseline. 

<img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmLORPLZTyzxUTW7k7WHnQ.jpeg">
Image: Compare prodcution and baseline to mixture

### Code example


```python
dist1 = [0.2, 0.5, 0.1, 0.3, 0.4]
dist2 = [0.3, 0.4, 0.1, 0.2, 0.3]

# calculate the js divergence
def js_divergence(d1, d2):
 m = 0.5 * (np.array(d1) + np.array(d2))
 return 0.5 * kl_divergence(d1, m) + 0.5 * kl_divergence(d2, m)

print("JS divergence in base e")
print("JS divergence (d1 || d2): {} nats".format(js_divergence(dist1,dist2)))
print("JS divergence (d2 || d1): {} nats".format(js_divergence(dist2,dist1)))
print("JS distance (d2 || d1): {}".format(np.sqrt(js_divergence(dist2,dist1))))

print("--------------------------")
print("JS divergence in base 2")
def kl_divergence_base2(d1, d2):
    return sum(d1[i] * np.log2(d1[i]/d2[i]) for i in range(len(d1)))
def js_divergence_base2(d1, d2):
 m = 0.5 * (np.array(d1) + np.array(d2))
 return 0.5 * kl_divergence_base2(d1, m) + 0.5 * kl_divergence_base2(d2, m)

print("JS divergence (d1 || d2): {} bits".format(js_divergence_base2(dist1,dist2)))
print("JS divergence (d2 || d1): {} bits".format(js_divergence_base2(dist2,dist1)))
print("JS distance (d2 || d1): {}".format(np.sqrt(js_divergence_base2(dist2,dist1))))

```

    JS divergence in base e
    JS divergence (d1 || d2): 0.016434955109340163 nats
    JS divergence (d2 || d1): 0.016434955109340163 nats
    JS distance (d2 || d1): 0.1281988888771668
    --------------------------
    JS divergence in base 2
    JS divergence (d1 || d2): 0.023710628233477783 bits
    JS divergence (d2 || d1): 0.023710628233477783 bits
    JS distance (d2 || d1): 0.153982558211889


# t-SNE

## Stochastic neighbor embedding

## t-SNE


```python

```
