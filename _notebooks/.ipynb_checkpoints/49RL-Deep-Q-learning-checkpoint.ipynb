{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf5123d",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"RL: Deep Q-learning\"\n",
    "date:   2023-06-16 10:14:54 +0700\n",
    "categories: DeepLearning\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Reinforcement Learning (RL) is the kind of learning that agent learns to make good decision from interacting with the environment. When interacting with the environment, agent receives reward or penalty and adjust their action based on that. This feedback system lets the agent learn from the experience and improve their decision making ability.\n",
    "\n",
    "There are several key components in reinforcement learning:\n",
    "\n",
    "- Agent: The learner/decision maker in the environment\n",
    "- Environment: The simulated world in which the agent operates\n",
    "- Action A: The possible moves that the agent can make. It can be discrete or continuous.\n",
    "- State S: The situation that the agent is in\n",
    "- Reward R: Feedback from the environment that guides the agent towards achieving goal. At each step, the cumulative reward equals the sum of all rewards in the future, with discount $$ R = \\sum_{k=0}^{\\infty} \\delta^k r_{t+k+1} $$\n",
    "- Policy $$ \\pi $$: A strategy/plan to determine action at each state\n",
    "\n",
    "The policy $$ \\pi $$ is the function that needs to be learned. It is the function that returns an action for any states in the process. The function will be trained to maximize the expected return. \n",
    "\n",
    "A policy based training algorithm would aim to return action at each state $$ a = \\pi(s) $$. In a stochastic setting, the output is a probability distribution over actions $$ \\pi(a\\mid s) = P(A\\mid s) $$\n",
    "\n",
    "A value based training algorithm learns a value function that maps each state to the expected value of that state. The expected and discount value of that state is the cash flow that the agent would get if follows the path from that state and each time choose the state with highest value.\n",
    "\n",
    "$$ v_{\\pi}(s) = E_{\\pi}{[R_{t+1} + \\delta R_{t+2} + \\delta^2 R_{t+3} + ... \\mid S_t = s ]} $$\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\">\n",
    "\n",
    "The goal of the agent is to maximize its cumulative reward (expected return). Sometimes a RL process can be considered a Markov Decision Process (MDP). A MDP cares only about the current state to decide the action, not all the history of all the states and actions. \n",
    "\n",
    "One thing to know about RL is that there needs to be a consideration between exploration and exploitation. Exploration is to explore the environment by trying random actions. This is to learn more about the environment in general. Exploitation is when we don't care about getting to know the environment, but use the what information we have to maximize the reward. Sometimes if we only do the exploitation (care about the immediate rewards), we miss the path leading to a big amount of reward down the line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bc664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
