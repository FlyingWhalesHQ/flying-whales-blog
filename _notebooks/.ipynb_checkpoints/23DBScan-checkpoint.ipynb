{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d788792",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Clustering: DBScan\"\n",
    "date:   2023-04-14 12:14:54 +0700\n",
    "categories: MachineLearning\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "Clustering is the task of grouping object into sub classes. DBScan is a density based algorithm to cluster dataset with noise. This allows looking for clusters of arbitrary shapes and no need to determine the number of clusters in the first place. Since for large dataset, it is very hard to know in advance those parameters.  \n",
    "\n",
    "Usually, we have two types of clustering algorithms: partitioning and hierarchical. Partitioning is when we construct a partition of database D of n objects into a set of k clusters. k is an input parameter which comes from domain knowledge which does not guarantee to be present in all cases. THe partitioning algorithm typically starts with an initial partition of D and then applies an iterative strategy to optimize an objective function. Each cluster is represented by its gravity center (k-means) or by one of the object near the center (k-medoids). This makes it a two step procedure: first determine k representatives that minimize the objective function, second assign objects to cluster with the closest center. The second step turns the partition into a Voronoi diagram, making the shape of each cluster convex. Hierarchical algorithm, on the other hand, decomposes the dataset D hierarchically, making a dendrogram out of it. A dendrogram is a tree that iteratively splits D into smaller subsets until each subset has only one data point. In such a scheme, each node of the tree is one cluster of D. The dendrogram can be created from the leaves up to the root (agglomerative approach) or vice versa (divisive approach). This method doesn't need a k as input. However it needs a termination condition. One condition is the distace $$ D_{min} $$ between all the clusters. So far the main challenge with the hierarchical clustering is deciding the termination condition, finding a $$ D_{min} $$ small enough to separate all supposed clusters, and large enough to not split any clusters into halves. \n",
    "\n",
    "\n",
    "# Density based cluster\n",
    "\n",
    "<img width=\"536\" alt=\"Screen Shot 2023-04-18 at 16 27 29\" src=\"https://user-images.githubusercontent.com/7457301/232735593-18805b89-19ed-4bc7-a14e-54b28dd94634.png\">\n",
    "\n",
    "Image: clusters that we can easily comprehend its density\n",
    "\n",
    "When we look at the sample databases in the image, we can intuitively see that density inside a cluster is high and density for noises are much lower. The author then formalize this intuition on defining characters of a cluster in a k-dimensional space S. This generalizes well to high dimensional space. The main idea is that for a point inside a cluster, there must be a minimum number of points in its neighborhood of a given radius. In other words, the density in the neighborhood has to exceed some threshold. The distance function is denoted d(p,q) for two points p and q.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
