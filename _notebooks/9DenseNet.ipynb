{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e3b97f",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Dense Neural Network\"\n",
    "date:   2023-03-08 10:14:54 +0700\n",
    "categories: jekyll update\n",
    "---\n",
    "\n",
    "# TOC\n",
    "\n",
    "- [Definition](#define)\n",
    "- [Backpropagation](#backprop)\n",
    "- [Gradient descent](#grad)\n",
    "- [Code example](#code)\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "Remember the linear combination of input x (note that x can be non linear):\n",
    "\n",
    "$$ \\hat{y}=h_{\\theta}(x) = \\theta \\cdot x $$ \n",
    "\n",
    "Also remember when we wrap this linear combination in all kinds of non linear function (sigmoid, sign, softmax). There are some other non linear functions that are also as popular: tanh, ReLU.. In general, those transformations are called activation functions. They are there to transform the data flow and to make the investigation intesresting (instead of a big chunk of linear combination) for complex problems.\n",
    "\n",
    "In deep learning, each of those nonlinear transformations is one neuron. Hence the perceptron has one neuron. Since it uses the sign function, we can call it a sign neuron. In general, the last neurons that output classes (using softmax) or values are called output layer. Those neurons between input and output layer are called hidden layers since they transform input and continue to do so before outputing some thing for classification or regression.\n",
    "\n",
    "This kind of network that each neuron of one layer is connected (to be input) to all the neuron for the next layer is called a dense network, or a fully connected feedforward network. It is called feedforward (or sequential) since the input flows (and is transformed) one-way forward from the input to output layer.\n",
    "\n",
    "## ReLU\n",
    "\n",
    "ReLU, shorted for Rectified linear unit, is an incredibly fast and straightforward but successful activation function. It is:\n",
    "\n",
    "$$ ReLU(x) = max(0, h_{\\theta}) $$\n",
    "\n",
    "ReLU returns either 0 or the linear combination of input, whichever is greater.\n",
    "\n",
    "## A 2-layer neural network\n",
    "\n",
    "A 2-layer neural network would have one hidden (middle) layer and one output layer. Let's begin the calculation. Say we have 3 attributes $$ x_1, x_2, x_3 $$, the linear combination would be:\n",
    "\n",
    "$$ \\hat{y_1} = h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 $$\n",
    "\n",
    "with $$ x_0 = 1 $$ and $$ \\theta_0 $$ to be the bias. Take this through the transformation of ReLU and we have the first neuron of the hidden layer:\n",
    "\n",
    "$$ a_{11} = ReLU(\\theta_{01} x_0 + \\theta_{11} x_1 + \\theta_{21} x_2 + \\theta_{31} x_3) $$\n",
    "\n",
    "For the second neuron of the hidden layers:\n",
    "\n",
    "$$ a_{21} = ReLU(\\theta_{02} x_0 + \\theta_{12} x_1 + \\theta_{22} x_2 + \\theta_{32} x_3) $$\n",
    "\n",
    "For the output layer:\n",
    "\n",
    "$$ \\hat{y} = h_\\theta(x) = \\theta_0 + \\theta_1 a_{11} + \\theta_2 a_{21}  $$\n",
    "\n",
    "Usually we don't use activation for the output layer that predicts a value (regression). If we need nonnegative value we can use ReLU. For classification problem, we can use a softmax.\n",
    "\n",
    "With this setup in general, we use a MSE for loss function of a regression problem and cross entropy for classification problem. To optimize loss function, we calculate gradient descent. Backpropagation is a technique to calculate gradient so we can use it for the descent step. In crucial, the whole process of training a neural network means:\n",
    "\n",
    "- to randomly initialize the parameter vector\n",
    "\n",
    "- use those starting parameters to do a forward calculation (multiply with input then transform) outputing prediction\n",
    "\n",
    "- measure the error of prediction\n",
    "\n",
    "- do a backward pass: calculate how much each parameter is responsible for the error (i.e. we take partial derivative of error with respect to each parameter since technically gradient measure how much pertubed the error is given a minor change in each paramater)\n",
    "\n",
    "- update the parameters in the direction of descending the gradient so that the error is on the way to the minimal\n",
    "\n",
    "The backward pass is called backpropagation: we backward propagate the error.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Here is the loss function:\n",
    "\n",
    "$$ L = \\frac{1}{2}(y - \\hat{y})^2 $$ \n",
    "\n",
    "Here is the derivatives of loss function with respect to parameters in the output layer $$ \\theta_0, \\theta_1, \\theta_2 $$:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_{0}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\theta_{0}} = (y - \\hat{y})(-\\hat{y}) \\frac{\\partial (\\theta_0 + \\theta_1 a_{11} + \\theta_2 a_{21})}{\\partial \\theta_0} = -\\hat{y}(y - \\hat{y}) $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\theta_{1}} = (y - \\hat{y})(-\\hat{y}) \\frac{\\partial (\\theta_0 + \\theta_1 a_{11} + \\theta_2 a_{21})}{\\partial \\theta_1} = -\\hat{y}(y - \\hat{y}) a_{11} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_2} = -\\hat{y}(y - \\hat{y})a_{21} $$\n",
    "\n",
    "Here is the derivatives of loss function with respect to parameters $$ \\theta_{01},..,\\theta_{31}, \\theta_{02},..,\\theta_{32} $$ in the hidden layer:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_{01}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial a_{11}} \\frac{\\partial a_{11}}{\\partial \\theta_{01}} = -\\hat{y}(y - \\hat{y}) \\theta_{1} \\frac{\\partial ReLU}{\\partial \\theta_{01}} $$\n",
    "\n",
    "with $$ \\frac{\\partial ReLU}{\\partial \\theta_{01}} = $$\n",
    "\n",
    "\\begin{cases}\n",
    "      0 & \\text{if $x_0$ < 0}\\\\\n",
    "      1 & \\text{if $x_0$ > 0}\\\\\n",
    "\\end{cases}\n",
    "\n",
    "$$ \\Leftrightarrow \\frac{\\partial L}{\\partial \\theta_{01}} = $$ \n",
    "\n",
    "\\begin{cases}\n",
    "      0 & \\text{if $x_0$ < 0}\\\\\n",
    "      -\\hat{y}(y - \\hat{y}) \\theta_{1} & \\text{if $x_0$ > 0}\\\\\n",
    "\\end{cases}\n",
    "\n",
    "## Code example\n",
    "\n",
    "Consider the following analytical example: 3 features and 2 input data points:\n",
    "\n",
    "| $$ x_1 $$ | $$ x_2$$  |  $$x_3  $$ | \n",
    "|--|--|--|\n",
    "| -2 | 4 | 3 |\n",
    "| 5  | 8 | 10|\n",
    "\n",
    "We have 11 parameters:\n",
    "\n",
    "|$$\\theta_{01}$$|$$\\theta_{11}$$|$$\\theta_{21}$$|$$\\theta_{31}$$| $$\\theta_{02}$$|$$\\theta_{12}$$|$$\\theta_{22}$$|$$\\theta_{32}$$| $$\\theta_0$$|$$\\theta_1$$|$$\\theta_2$$ |\n",
    "|--|--|--|--|--|--|--|--|--|--|--|\n",
    "|4|7|8|2|4|5|7|9|1|10|3|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d72986",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[-2,4,3],[5,8,10]]\n",
    "theta = [[4,7,8,2],[4,5,7,9]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aca7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
